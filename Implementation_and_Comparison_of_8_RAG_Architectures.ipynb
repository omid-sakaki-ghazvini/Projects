{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installation & Setup**"
      ],
      "metadata": {
        "id": "HkdczSJRokni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core packages\n",
        "!pip install -qU langchain langchain-community langchain-experimental transformers sentence-transformers faiss-cpu rank_bm25 llama-cpp-python\n",
        "\n",
        "# Install additional packages for GraphRAG and data handling\n",
        "!pip install -qU networkx scipy pandas\n",
        "\n",
        "# Download the quantized LLM model\n",
        "!wget -q -O llama-2-7b-chat.Q4_0.gguf https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf"
      ],
      "metadata": {
        "id": "2hsE0oHCe-K_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Data Preparation & Common Setup**"
      ],
      "metadata": {
        "id": "ZnPqPMcDorFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Sample financial news corpus\n",
        "corpus = [\n",
        "    \"Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\",\n",
        "    \"The Federal Reserve is expected to hold interest rates steady at 5.5% in its upcoming meeting, according to economists.\",\n",
        "    \"Tesla's stock price fell by 6% after the company missed delivery estimates for Q4 2023. The company delivered 485,000 vehicles.\",\n",
        "    \"Microsoft announced a new $50 billion stock buyback program and increased its dividend by 10%. CEO Satya Nadella praised the company's cloud growth.\",\n",
        "    \"The global semiconductor shortage is projected to ease by mid-2024, as new manufacturing capacity comes online, says a report from Gartner.\",\n",
        "    \"Amazon Web Services (AWS) signed a $1 billion contract with a major enterprise client, highlighting the continued growth of cloud computing.\",\n",
        "    \"The SEC approved the first spot Bitcoin ETFs, a landmark decision for cryptocurrency integration with traditional finance.\",\n",
        "    \"Unemployment claims in the US dropped to 210,000 last week, signaling a resilient labor market amidst economic uncertainties.\",\n",
        "    \"Oil prices surged by 4% to $85 per barrel following geopolitical tensions in the Middle East and production cuts by OPEC+.\",\n",
        "    \"The Bank of Japan maintained its ultra-loose monetary policy, keeping interest rates negative and yield caps in place for now.\"\n",
        "]\n",
        "\n",
        "# Test question\n",
        "test_question = \"What was Apple's quarterly revenue?\"\n",
        "\n",
        "print(f\"Corpus size: {len(corpus)} documents\")\n",
        "print(f\"Test question: {test_question}\")\n",
        "\n",
        "# Create Documents\n",
        "docs = [Document(page_content=text) for text in corpus]\n",
        "\n",
        "# Split documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# Initialize Embeddings\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Create a central FAISS vector store for most architectures\n",
        "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Top 3 results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pkcbAvCfJwQ",
        "outputId": "789b6941-e9ec-4998-872f-a25fc6a5dd38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus size: 10 documents\n",
            "Test question: What was Apple's quarterly revenue?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4231445915.py:40: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the quantized LLM\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"llama-2-7b-chat.Q4_0.gguf\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512,\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        "    n_ctx=4096,\n",
        "    n_gpu_layers=40,\n",
        "    n_batch=512,\n",
        "    stop=[\"</s>\", \"USER:\", \"ASSISTANT:\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb6soG6PfWpI",
        "outputId": "c13ba7c7-4b74-48e5-a073-5dc5cf02e85c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 3.56 GiB (4.54 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 6.74 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_0) (and 66 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =  3474.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  3647.87 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
            "repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
            "repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
            ".repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
            "....\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2328\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   312.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Define the Architectures**"
      ],
      "metadata": {
        "id": "xYKUzwOio8HR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "g1pn_ykhfZ_H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1. Naive RAG**"
      ],
      "metadata": {
        "id": "kbf-pOzapAiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_rag(query):\n",
        "    \"\"\"Standard RAG: Retrieve relevant chunks and generate an answer.\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        You are a helpful, respectful, and honest financial assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "        <</SYS>>\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Provide a concise and accurate answer: [/INST]\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Retrieve context\n",
        "    retrieved_docs = retriever.invoke(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Generate answer\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "    answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
        "    return answer, context"
      ],
      "metadata": {
        "id": "o9czHTC7faoo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2. HyDE**"
      ],
      "metadata": {
        "id": "UbSvH9NZpHCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hyde_rag(query):\n",
        "    \"\"\"Hypothetical Document Embeddings: Generate a hypothetical answer first, then use it for retrieval.\"\"\"\n",
        "    # Step 1: Generate a hypothetical answer\n",
        "    hyde_prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        You are an expert financial analyst. Write a hypothetical paragraph that answers the following question. The paragraph should be informative and contain key entities and facts.\n",
        "        <</SYS>>\n",
        "        Question: {question}\n",
        "        Hypothetical Answer: [/INST]\"\"\",\n",
        "        input_variables=[\"question\"]\n",
        "    )\n",
        "    hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
        "    hypothetical_doc = hyde_chain.invoke({\"question\": query})\n",
        "\n",
        "    # Step 2: Use the hypothetical doc for retrieval\n",
        "    hypothetical_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "    retrieved_docs = hypothetical_retriever.invoke(hypothetical_doc)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Step 3: Generate the final answer\n",
        "    answer_prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        You are a helpful assistant. Use the following context to answer the question.\n",
        "        <</SYS>>\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Concise Answer: [/INST]\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    answer_chain = answer_prompt | llm | StrOutputParser()\n",
        "    answer = answer_chain.invoke({\"context\": context, \"question\": query})\n",
        "    return answer, context, hypothetical_doc"
      ],
      "metadata": {
        "id": "Q0_Y5Zc0fizO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3. Corrective RAG (CRAG) - Simplified**"
      ],
      "metadata": {
        "id": "bQBR0hu6pMng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crag_rag(query):\n",
        "    \"\"\"Corrective RAG: Evaluate the retrieved documents before generating an answer.\"\"\"\n",
        "    # Retrieve context\n",
        "    retrieved_docs = retriever.invoke(query)\n",
        "    context_text = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # **Simplified Corrective Step**: Use LLM to evaluate context relevance\n",
        "    correction_prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        Evaluate if the following retrieved context is relevant and sufficient to answer the question. Answer ONLY 'YES' or 'NO'.\n",
        "        <</SYS>>\n",
        "        Question: {question}\n",
        "        Context: {context}\n",
        "        Evaluation: [/INST]\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    correction_chain = correction_prompt | llm | StrOutputParser()\n",
        "    evaluation = correction_chain.invoke({\"context\": context_text, \"question\": query})\n",
        "\n",
        "    if \"YES\" in evaluation.upper():\n",
        "        # If context is good, proceed with standard RAG\n",
        "        answer_prompt = PromptTemplate(\n",
        "            template=\"\"\"<s>[INST] <<SYS>>\n",
        "            Answer the question using the context.\n",
        "            <</SYS>>\n",
        "            Context: {context}\n",
        "            Question: {question}\n",
        "            Answer: [/INST]\"\"\",\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "        answer_chain = answer_prompt | llm | StrOutputParser()\n",
        "        answer = answer_chain.invoke({\"context\": context_text, \"question\": query})\n",
        "        corrective_action = \"Proceeded with standard generation.\"\n",
        "    else:\n",
        "        # If context is bad, use web search (fallback) - Simplified here to just use LLM knowledge\n",
        "        corrective_action = \"Context deemed insufficient. Using LLM's internal knowledge (fallback mode).\"\n",
        "        fallback_prompt = PromptTemplate(\n",
        "            template=\"\"\"<s>[INST] <<SYS>>\n",
        "            You are a knowledgeable assistant. Answer the question based on your own knowledge.\n",
        "            <</SYS>>\n",
        "            Question: {question}\n",
        "            Answer: [/INST]\"\"\",\n",
        "            input_variables=[\"question\"]\n",
        "        )\n",
        "        fallback_chain = fallback_prompt | llm | StrOutputParser()\n",
        "        answer = fallback_chain.invoke({\"question\": query})\n",
        "        context_text = \"N/A - Used internal knowledge\"\n",
        "\n",
        "    return answer, context_text, corrective_action"
      ],
      "metadata": {
        "id": "8z-p_ojFfnXo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4. GraphRAG - Simplified (Using TextRank-like concept)**"
      ],
      "metadata": {
        "id": "oBdwrWR2pSDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def build_graph_from_docs(documents, embeddings_model, similarity_threshold=0.7):\n",
        "    \"\"\"Build a simple knowledge graph from documents based on semantic similarity.\"\"\"\n",
        "    G = nx.Graph()\n",
        "    text_list = [doc.page_content for doc in documents]\n",
        "    # Add nodes\n",
        "    for i, text in enumerate(text_list):\n",
        "        G.add_node(i, text=text)\n",
        "\n",
        "    # Get embeddings for all documents\n",
        "    doc_embeddings = embeddings_model.embed_documents(text_list)\n",
        "    doc_embeddings = np.array(doc_embeddings)\n",
        "\n",
        "    # Add edges based on cosine similarity\n",
        "    for i in range(len(doc_embeddings)):\n",
        "        for j in range(i+1, len(doc_embeddings)):\n",
        "            sim = cosine_similarity([doc_embeddings[i]], [doc_embeddings[j]])[0][0]\n",
        "            if sim > similarity_threshold:\n",
        "                G.add_edge(i, j, weight=sim)\n",
        "    return G\n",
        "\n",
        "def graphrag_rag(query, all_docs=split_docs):\n",
        "    \"\"\"GraphRAG: Retrieve information using a knowledge graph.\"\"\"\n",
        "    # Build graph (in a real scenario, this would be pre-built)\n",
        "    G = build_graph_from_docs(all_docs, embeddings)\n",
        "\n",
        "    # Embed the query\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    query_embedding = np.array(query_embedding)\n",
        "\n",
        "    # Find the most relevant node to the query\n",
        "    doc_embeddings = embeddings.embed_documents([doc.page_content for doc in all_docs])\n",
        "    doc_embeddings = np.array(doc_embeddings)\n",
        "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "    most_relevant_node_idx = np.argmax(similarities)\n",
        "\n",
        "    # Get the context from the most relevant node and its neighbors\n",
        "    context_nodes = [most_relevant_node_idx]\n",
        "    context_nodes.extend(list(G.neighbors(most_relevant_node_idx)))\n",
        "    context = \"\\n\".join([all_docs[i].page_content for i in context_nodes])\n",
        "\n",
        "    # Generate answer\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        Use the following context from a knowledge graph to answer the question.\n",
        "        <</SYS>>\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Answer: [/INST]\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "    answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
        "    return answer, context"
      ],
      "metadata": {
        "id": "nHIQcUTMfrbw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.5. Multimodal RAG (Basic Text + Table Understanding)**"
      ],
      "metadata": {
        "id": "8oC7SrRXpXRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_text = \"| Company | Q1 2024 Revenue | Net Profit |\\n|---------|----------------|------------|\\n| Apple | $123.9B | $34.6B |\\n| Microsoft | $62.0B | $21.9B |\\n| Tesla | $25.1B | $2.7B |\"\n",
        "multimodal_corpus = corpus + [table_text]\n",
        "multimodal_docs = [Document(page_content=text) for text in multimodal_corpus]\n",
        "multimodal_split_docs = text_splitter.split_documents(multimodal_docs)\n",
        "multimodal_vectorstore = FAISS.from_documents(multimodal_split_docs, embeddings)\n",
        "multimodal_retriever = multimodal_vectorstore.as_retriever(search_kwargs={\"k\": 4}) # Retrieve more chunks\n",
        "\n",
        "def multimodal_rag(query):\n",
        "    \"\"\"Multimodal RAG: Handles both textual and tabular data.\"\"\"\n",
        "    retrieved_docs = multimodal_retriever.invoke(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        You are an assistant skilled in analyzing both text and tables. Use the following context, which may contain text paragraphs or markdown tables, to answer the question.\n",
        "        <</SYS>>\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Provide a precise answer: [/INST]\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "    answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
        "    return answer, context"
      ],
      "metadata": {
        "id": "Lq8rAUX2fvzQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.6. Hybrid RAG (Dense + Sparse Retrieval)**"
      ],
      "metadata": {
        "id": "AmHkkFOtpcPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "class HybridRetriever:\n",
        "    def __init__(self, vector_retriever, text_list):\n",
        "        self.vector_retriever = vector_retriever\n",
        "        self.bm25 = BM25Okapi([text.split() for text in text_list])\n",
        "\n",
        "    def retrieve(self, query, k=3):\n",
        "        # Dense Retrieval\n",
        "        dense_docs = self.vector_retriever.invoke(query)\n",
        "        dense_context = \"\\n\".join([doc.page_content for doc in dense_docs])\n",
        "\n",
        "        # Sparse Retrieval (BM25)\n",
        "        tokenized_query = query.split()\n",
        "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_bm25_indices = np.argsort(bm25_scores)[::-1][:k]\n",
        "        sparse_context = \"\\n\".join([corpus[i] for i in top_bm25_indices])\n",
        "\n",
        "        # Combine contexts\n",
        "        hybrid_context = f\"[DENSE RETRIEVAL RESULTS]:\\n{dense_context}\\n\\n[SPARSE RETRIEVAL RESULTS]:\\n{sparse_context}\"\n",
        "        return hybrid_context\n",
        "\n",
        "# Initialize\n",
        "text_list_for_bm25 = [doc.page_content for doc in split_docs]\n",
        "hybrid_retriever_obj = HybridRetriever(retriever, text_list_for_bm25)\n",
        "\n",
        "def hybrid_rag(query):\n",
        "    \"\"\"Hybrid RAG: Combines dense (vector) and sparse (BM25) retrieval.\"\"\"\n",
        "    context = hybrid_retriever_obj.retrieve(query)\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        Use the following context from two different retrieval methods to answer the question.\n",
        "        <</SYS>>\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Answer: [/INST]\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "    answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
        "    return answer, context"
      ],
      "metadata": {
        "id": "EtaVruupf0LA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.7. Adaptive RAG**"
      ],
      "metadata": {
        "id": "UUt_9upOpfpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_rag(query):\n",
        "    \"\"\"Adaptive RAG: Decides whether to retrieve or not based on the query.\"\"\"\n",
        "    # Decision Prompt\n",
        "    decision_prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        Decide if the following question requires retrieving external information from a database of financial news to be answered accurately. Answer ONLY 'YES' or 'NO'.\n",
        "        <</SYS>>\n",
        "        Question: {question}\n",
        "        Decision: [/INST]\"\"\",\n",
        "        input_variables=[\"question\"]\n",
        "    )\n",
        "    decision_chain = decision_prompt | llm | StrOutputParser()\n",
        "    decision = decision_chain.invoke({\"question\": query})\n",
        "\n",
        "    if \"YES\" in decision.upper():\n",
        "        # Use Naive RAG\n",
        "        retrieved_docs = retriever.invoke(query)\n",
        "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "        action = \"Decision: Retrieval used.\"\n",
        "    else:\n",
        "        # Answer from internal knowledge\n",
        "        context = \"N/A\"\n",
        "        action = \"Decision: No retrieval needed.\"\n",
        "\n",
        "    # Generate answer (use the same prompt for both for fairness)\n",
        "    answer_prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        You are a helpful assistant. Use the context if provided to answer the question. If no context is provided, use your own knowledge.\n",
        "        <</SYS>>\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Answer: [/INST]\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    answer_chain = answer_prompt | llm | StrOutputParser()\n",
        "    answer = answer_chain.invoke({\"context\": context, \"question\": query})\n",
        "    return answer, context, action"
      ],
      "metadata": {
        "id": "WpUzUe6Af3gR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.8. Agentic RAG - Simplified (Single Agent with Plan-and-Act)**"
      ],
      "metadata": {
        "id": "HViCzLLgpjJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agentic_rag(query):\n",
        "    \"\"\"Agentic RAG: Uses a planning step to break down the query before retrieval and generation.\"\"\"\n",
        "    # Planning Step\n",
        "    planner_prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        You are a query planner. Break down the following complex question into 2-3 simpler, more specific sub-questions that would help retrieve relevant information. List them one per line.\n",
        "        <</SYS>>\n",
        "        Complex Question: {question}\n",
        "        Sub-questions: [/INST]\"\"\",\n",
        "        input_variables=[\"question\"]\n",
        "    )\n",
        "    planner_chain = planner_prompt | llm | StrOutputParser()\n",
        "    sub_questions_text = planner_chain.invoke({\"question\": query})\n",
        "    sub_questions = [q.strip() for q in sub_questions_text.split('\\n') if q.strip()]\n",
        "\n",
        "    # Act Step: Retrieve for each sub-question\n",
        "    collected_context = \"\"\n",
        "    for sub_q in sub_questions:\n",
        "        retrieved_docs = retriever.invoke(sub_q)\n",
        "        context_for_sub_q = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "        collected_context += f\"\\n\\n# Information relevant to sub-question: '{sub_q}'\\n{context_for_sub_q}\"\n",
        "\n",
        "    # Final Answer Generation\n",
        "    synthesizer_prompt = PromptTemplate(\n",
        "        template=\"\"\"<s>[INST] <<SYS>>\n",
        "        You are a synthesis agent. You have broken down a complex question into parts and retrieved information for each part. Synthesize all the information below into a comprehensive and accurate final answer for the original question.\n",
        "        <</SYS>>\n",
        "        Original Question: {question}\n",
        "        Retrieved Information for all sub-questions: {context}\n",
        "        Comprehensive Final Answer: [/INST]\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    synthesizer_chain = synthesizer_prompt | llm | StrOutputParser()\n",
        "    final_answer = synthesizer_chain.invoke({\"context\": collected_context, \"question\": query})\n",
        "    return final_answer, collected_context, sub_questions"
      ],
      "metadata": {
        "id": "Wlw-s0eof6tI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Run the Comparison**"
      ],
      "metadata": {
        "id": "R4En-wMLpnhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize results list for single question\n",
        "results = []\n",
        "\n",
        "# Function to run a single architecture and store results\n",
        "def run_architecture(name, func, has_extra_info=False):\n",
        "    print(f\"\\n--- Running {name} ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        if has_extra_info:\n",
        "            answer, context, extra_info = func(test_question)\n",
        "        else:\n",
        "            answer, context = func(test_question)\n",
        "            extra_info = \"N/A\"\n",
        "    except Exception as e:\n",
        "        answer = f\"Error: {str(e)}\"\n",
        "        context = \"N/A\"\n",
        "        extra_info = \"N/A\"\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = round(end_time - start_time, 2)\n",
        "\n",
        "    results.append({\n",
        "        'Architecture': name,\n",
        "        'Answer': answer,\n",
        "        'Context Used': context[:500] + \"...\" if len(context) > 500 else context, # Truncate long context\n",
        "        'Extra Info': extra_info,\n",
        "        'Time (s)': elapsed_time\n",
        "    })\n",
        "\n",
        "    print(f\"Completed in {elapsed_time}s\")\n",
        "\n",
        "# Run all architectures\n",
        "run_architecture(\"Naive RAG\", naive_rag)\n",
        "run_architecture(\"HyDE\", hyde_rag, has_extra_info=True)\n",
        "run_architecture(\"Corrective RAG\", crag_rag, has_extra_info=True)\n",
        "run_architecture(\"GraphRAG\", graphrag_rag)\n",
        "run_architecture(\"Multimodal RAG\", multimodal_rag)\n",
        "run_architecture(\"Hybrid RAG\", hybrid_rag)\n",
        "run_architecture(\"Adaptive RAG\", adaptive_rag, has_extra_info=True)\n",
        "run_architecture(\"Agentic RAG\", agentic_rag, has_extra_info=True)\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnlUsbbIgB74",
        "outputId": "672c9d14-5011-4aa3-8730-8cb326c7c7de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Naive RAG ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Thank you for the question! Based on the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   48668.76 ms /   206 tokens (  236.26 ms per token,     4.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =   22030.54 ms /    37 runs   (  595.42 ms per token,     1.68 tokens per second)\n",
            "llama_perf_context_print:       total time =   70782.41 ms /   243 tokens\n",
            "llama_perf_context_print:    graphs reused =         35\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 13 prefix-match hit, remaining 64 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 70.83s\n",
            "\n",
            "--- Running HyDE ---\n",
            "  As an expert financial analyst, I can tell you that Apple's quarterly revenue for the most recent quarter (Q4 of fiscal year 2023) was $71.5 billion. This represents a 12% increase from the same quarter the previous year, driven by strong demand for the company's iPhones, Macs, and other products. The revenue figure includes both hardware and services revenue, with the iPhone segment generating $34.6 billion in revenue, up 10% from the same period last year. The Mac segment contributed $7.2 billion in revenue, a 25% increase from the same quarter last year. Other products, including Apple Watch, AirPods, and HomePod, generated $2.3 billion in revenue, a 46% increase from the same quarter last year. Services revenue, which includes the App Store, Apple Music, and Apple TV+, reached $10.8 billion, a 15% increase from the same quarter last year. Overall, Apple's strong performance in Q4 reflects the company's continued ability to innovate and deliver high-quality products and services that resonate with customers around the world."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   12876.67 ms /    64 tokens (  201.20 ms per token,     4.97 tokens per second)\n",
            "llama_perf_context_print:        eval time =  154608.80 ms /   263 runs   (  587.87 ms per token,     1.70 tokens per second)\n",
            "llama_perf_context_print:       total time =  168124.96 ms /   327 tokens\n",
            "llama_perf_context_print:    graphs reused =        254\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 13 prefix-match hit, remaining 169 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  According to the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   30692.37 ms /   169 tokens (  181.61 ms per token,     5.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =   18038.74 ms /    31 runs   (  581.89 ms per token,     1.72 tokens per second)\n",
            "llama_perf_context_print:       total time =   48801.39 ms /   200 tokens\n",
            "llama_perf_context_print:    graphs reused =         29\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 11 prefix-match hit, remaining 174 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 217.2s\n",
            "\n",
            "--- Running Corrective RAG ---\n",
            "  YES"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   33012.83 ms /   174 tokens (  189.73 ms per token,     5.27 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1145.98 ms /     2 runs   (  572.99 ms per token,     1.75 tokens per second)\n",
            "llama_perf_context_print:       total time =   34163.44 ms /   176 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 10 prefix-match hit, remaining 153 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  According to the context, Apple's quarterly revenue for Q1 2024 was $123.9 billion."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   28548.03 ms /   153 tokens (  186.59 ms per token,     5.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =   17905.34 ms /    30 runs   (  596.84 ms per token,     1.68 tokens per second)\n",
            "llama_perf_context_print:       total time =   46531.61 ms /   183 tokens\n",
            "llama_perf_context_print:    graphs reused =         28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 80.73s\n",
            "\n",
            "--- Running GraphRAG ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 10 prefix-match hit, remaining 89 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Based on the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   16709.00 ms /    89 tokens (  187.74 ms per token,     5.33 tokens per second)\n",
            "llama_perf_context_print:        eval time =   17879.16 ms /    31 runs   (  576.75 ms per token,     1.73 tokens per second)\n",
            "llama_perf_context_print:       total time =   34657.11 ms /   120 tokens\n",
            "llama_perf_context_print:    graphs reused =         29\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 11 prefix-match hit, remaining 267 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 34.89s\n",
            "\n",
            "--- Running Multimodal RAG ---\n",
            "  Based on the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   49345.97 ms /   267 tokens (  184.82 ms per token,     5.41 tokens per second)\n",
            "llama_perf_context_print:        eval time =   18588.62 ms /    31 runs   (  599.63 ms per token,     1.67 tokens per second)\n",
            "llama_perf_context_print:       total time =   68012.98 ms /   298 tokens\n",
            "llama_perf_context_print:    graphs reused =         29\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 11 prefix-match hit, remaining 301 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 68.05s\n",
            "\n",
            "--- Running Hybrid RAG ---\n",
            "  Based on the context provided, Apple's quarterly revenue was $123.9 billion for Q1 2024."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   54536.01 ms /   301 tokens (  181.18 ms per token,     5.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =   18560.43 ms /    31 runs   (  598.72 ms per token,     1.67 tokens per second)\n",
            "llama_perf_context_print:       total time =   73168.50 ms /   332 tokens\n",
            "llama_perf_context_print:    graphs reused =         29\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 11 prefix-match hit, remaining 63 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 73.2s\n",
            "\n",
            "--- Running Adaptive RAG ---\n",
            "  YES"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   12062.90 ms /    63 tokens (  191.47 ms per token,     5.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1468.92 ms /     2 runs   (  734.46 ms per token,     1.36 tokens per second)\n",
            "llama_perf_context_print:       total time =   13539.30 ms /    65 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 11 prefix-match hit, remaining 172 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Based on the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   30715.00 ms /   172 tokens (  178.58 ms per token,     5.60 tokens per second)\n",
            "llama_perf_context_print:        eval time =   18038.60 ms /    31 runs   (  581.89 ms per token,     1.72 tokens per second)\n",
            "llama_perf_context_print:       total time =   48816.73 ms /   203 tokens\n",
            "llama_perf_context_print:    graphs reused =         29\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 14 prefix-match hit, remaining 67 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 62.4s\n",
            "\n",
            "--- Running Agentic RAG ---\n",
            "  Sure! Here are 3 simpler sub-questions that can help retrieve relevant information about Apple's quarterly revenue:\n",
            "1. What was Apple's total revenue for the most recent completed quarter?\n",
            "2. How did Apple's quarterly revenue compare to the same quarter in the previous year?\n",
            "3. Which product categories or segments of Apple's business generated the highest revenue during the most recent quarter?"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =   12780.28 ms /    67 tokens (  190.75 ms per token,     5.24 tokens per second)\n",
            "llama_perf_context_print:        eval time =   52848.29 ms /    90 runs   (  587.20 ms per token,     1.70 tokens per second)\n",
            "llama_perf_context_print:       total time =   65834.35 ms /   157 tokens\n",
            "llama_perf_context_print:    graphs reused =         86\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "Llama.generate: 14 prefix-match hit, remaining 679 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Based on the information retrieved, Apple's quarterly revenue for Q1 2024 was $123.9 billion, driven by strong iPhone sales. This represents a record quarterly revenue for Apple and an increase of 31% compared to the same quarter in the previous year.\n",
            "In terms of product categories or segments, iPhone sales generated the highest revenue during the most recent quarter, followed by Services and Mac. The company's net profit was $34.6 billion, an increase of 37% compared to the same quarter in the previous year.\n",
            "Microsoft also had a strong quarter, with a new $50 billion stock buyback program and an increased dividend by 10%. CEO Satya Nadella praised the company's cloud growth, highlighting the continued success of its cloud computing segment.\n",
            "Finally, Amazon Web Services (AWS) signed a $1 billion contract with a major enterprise client, further solidifying its position as a leader in the cloud computing market.\n",
            "In summary, Apple and Microsoft had strong quarters, driven by their respective product categories and segments, while AWS continued to grow its cloud computing business through strategic partnerships and contracts."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   48669.46 ms\n",
            "llama_perf_context_print: prompt eval time =  125177.57 ms /   679 tokens (  184.36 ms per token,     5.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =  162158.74 ms /   256 runs   (  633.43 ms per token,     1.58 tokens per second)\n",
            "llama_perf_context_print:       total time =  287965.67 ms /   935 tokens\n",
            "llama_perf_context_print:    graphs reused =        247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 353.93s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Display Results**"
      ],
      "metadata": {
        "id": "YbQ46eRrpvj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the Results\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"COMPARATIVE RESULTS\")\n",
        "print(\"=\"*100)\n",
        "print(f\"Question: {test_question}\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Display in a clean table format\n",
        "print(tabulate(results_df[['Architecture', 'Answer', 'Time (s)']], headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "# Display detailed context and extra info\n",
        "print(\"\\n\\n\" + \"=\"*100)\n",
        "print(\"DETAILED CONTEXT AND EXTRA INFORMATION\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for _, row in results_df.iterrows():\n",
        "    print(f\"\\n--- {row['Architecture']} ---\")\n",
        "    print(f\"Context: {row['Context Used']}\")\n",
        "    if row['Extra Info'] != \"N/A\":\n",
        "        print(f\"Extra Info: {row['Extra Info']}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIGmdO2ZgE_X",
        "outputId": "5c5b0205-54a7-4363-9fe0-88d8b67c2661"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "COMPARATIVE RESULTS\n",
            "====================================================================================================\n",
            "Question: What was Apple's quarterly revenue?\n",
            "====================================================================================================\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "| Architecture   | Answer                                                                                                                                                                                                                                                                 |   Time (s) |\n",
            "+================+========================================================================================================================================================================================================================================================================+============+\n",
            "| Naive RAG      | Thank you for the question! Based on the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion.                                                                                                                                                   |      70.83 |\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "| HyDE           | According to the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion.                                                                                                                                                                           |     217.2  |\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "| Corrective RAG | According to the context, Apple's quarterly revenue for Q1 2024 was $123.9 billion.                                                                                                                                                                                    |      80.73 |\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "| GraphRAG       | Based on the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion.                                                                                                                                                                               |      34.89 |\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "| Multimodal RAG | Based on the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion.                                                                                                                                                                               |      68.05 |\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "| Hybrid RAG     | Based on the context provided, Apple's quarterly revenue was $123.9 billion for Q1 2024.                                                                                                                                                                               |      73.2  |\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "| Adaptive RAG   | Based on the context provided, Apple's quarterly revenue for Q1 2024 was $123.9 billion.                                                                                                                                                                               |      62.4  |\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "| Agentic RAG    | Based on the information retrieved, Apple's quarterly revenue for Q1 2024 was $123.9 billion, driven by strong iPhone sales. This represents a record quarterly revenue for Apple and an increase of 31% compared to the same quarter in the previous year.            |     353.93 |\n",
            "|                | In terms of product categories or segments, iPhone sales generated the highest revenue during the most recent quarter, followed by Services and Mac. The company's net profit was $34.6 billion, an increase of 37% compared to the same quarter in the previous year. |            |\n",
            "|                | Microsoft also had a strong quarter, with a new $50 billion stock buyback program and an increased dividend by 10%. CEO Satya Nadella praised the company's cloud growth, highlighting the continued success of its cloud computing segment.                           |            |\n",
            "|                | Finally, Amazon Web Services (AWS) signed a $1 billion contract with a major enterprise client, further solidifying its position as a leader in the cloud computing market.                                                                                            |            |\n",
            "|                | In summary, Apple and Microsoft had strong quarters, driven by their respective product categories and segments, while AWS continued to grow its cloud computing business through strategic partnerships and contracts.                                                |            |\n",
            "+----------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "DETAILED CONTEXT AND EXTRA INFORMATION\n",
            "====================================================================================================\n",
            "\n",
            "--- Naive RAG ---\n",
            "Context: Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\n",
            "Microsoft announced a new $50 billion stock buyback program and increased its dividend by 10%. CEO Satya Nadella praised the company's cloud growth.\n",
            "Amazon Web Services (AWS) signed a $1 billion contract with a major enterprise client, highlighting the continued growth of cloud computing.\n",
            "\n",
            "\n",
            "--- HyDE ---\n",
            "Context: Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\n",
            "Microsoft announced a new $50 billion stock buyback program and increased its dividend by 10%. CEO Satya Nadella praised the company's cloud growth.\n",
            "Tesla's stock price fell by 6% after the company missed delivery estimates for Q4 2023. The company delivered 485,000 vehicles.\n",
            "Extra Info:   As an expert financial analyst, I can tell you that Apple's quarterly revenue for the most recent quarter (Q4 of fiscal year 2023) was $71.5 billion. This represents a 12% increase from the same quarter the previous year, driven by strong demand for the company's iPhones, Macs, and other products. The revenue figure includes both hardware and services revenue, with the iPhone segment generating $34.6 billion in revenue, up 10% from the same period last year. The Mac segment contributed $7.2 billion in revenue, a 25% increase from the same quarter last year. Other products, including Apple Watch, AirPods, and HomePod, generated $2.3 billion in revenue, a 46% increase from the same quarter last year. Services revenue, which includes the App Store, Apple Music, and Apple TV+, reached $10.8 billion, a 15% increase from the same quarter last year. Overall, Apple's strong performance in Q4 reflects the company's continued ability to innovate and deliver high-quality products and services that resonate with customers around the world.\n",
            "\n",
            "\n",
            "--- Corrective RAG ---\n",
            "Context: Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\n",
            "Microsoft announced a new $50 billion stock buyback program and increased its dividend by 10%. CEO Satya Nadella praised the company's cloud growth.\n",
            "Amazon Web Services (AWS) signed a $1 billion contract with a major enterprise client, highlighting the continued growth of cloud computing.\n",
            "Extra Info: Proceeded with standard generation.\n",
            "\n",
            "\n",
            "--- GraphRAG ---\n",
            "Context: Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\n",
            "\n",
            "\n",
            "--- Multimodal RAG ---\n",
            "Context: Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\n",
            "| Company | Q1 2024 Revenue | Net Profit |\n",
            "|---------|----------------|------------|\n",
            "| Apple | $123.9B | $34.6B |\n",
            "| Microsoft | $62.0B | $21.9B |\n",
            "| Tesla | $25.1B | $2.7B |\n",
            "Microsoft announced a new $50 billion stock buyback program and increased its dividend by 10%. CEO Satya Nadella praised the company's cloud growth.\n",
            "Amazon Web Services (AWS) signed a $1 bil...\n",
            "\n",
            "\n",
            "--- Hybrid RAG ---\n",
            "Context: [DENSE RETRIEVAL RESULTS]:\n",
            "Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\n",
            "Microsoft announced a new $50 billion stock buyback program and increased its dividend by 10%. CEO Satya Nadella praised the company's cloud growth.\n",
            "Amazon Web Services (AWS) signed a $1 billion contract with a major enterprise client, highlighting the continued growth of cloud computing.\n",
            "\n",
            "[SPARSE RETRIEVAL RESULTS]:\n",
            "Apple Inc. report...\n",
            "\n",
            "\n",
            "--- Adaptive RAG ---\n",
            "Context: Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\n",
            "Microsoft announced a new $50 billion stock buyback program and increased its dividend by 10%. CEO Satya Nadella praised the company's cloud growth.\n",
            "Amazon Web Services (AWS) signed a $1 billion contract with a major enterprise client, highlighting the continued growth of cloud computing.\n",
            "Extra Info: Decision: Retrieval used.\n",
            "\n",
            "\n",
            "--- Agentic RAG ---\n",
            "Context: \n",
            "\n",
            "# Information relevant to sub-question: 'Sure! Here are 3 simpler sub-questions that can help retrieve relevant information about Apple's quarterly revenue:'\n",
            "Apple Inc. reported record quarterly revenue of $123.9 billion for Q1 2024, driven by strong iPhone sales. Net profit was $34.6 billion.\n",
            "Microsoft announced a new $50 billion stock buyback program and increased its dividend by 10%. CEO Satya Nadella praised the company's cloud growth.\n",
            "The SEC approved the first spot Bitcoin ETFs, a landma...\n",
            "Extra Info: [\"Sure! Here are 3 simpler sub-questions that can help retrieve relevant information about Apple's quarterly revenue:\", \"1. What was Apple's total revenue for the most recent completed quarter?\", \"2. How did Apple's quarterly revenue compare to the same quarter in the previous year?\", \"3. Which product categories or segments of Apple's business generated the highest revenue during the most recent quarter?\"]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}