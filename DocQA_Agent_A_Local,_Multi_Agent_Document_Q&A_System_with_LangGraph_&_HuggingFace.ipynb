{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/DRPYyBR8KaJezuCbyAGF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q langgraph langchain langchain-community faiss-cpu pypdf langchain-core wikipedia arxiv openai sentence-transformers"
      ],
      "metadata": {
        "id": "0Vwd4lOL-jmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FUPphZbD_aoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict, Any\n",
        "from langgraph.graph import Graph, END\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "VzWce4FE3qSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Initialize the LLM using HuggingFace pipeline (local execution, NO InferenceClient)**"
      ],
      "metadata": {
        "id": "ViZGbrsU4EAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=256,\n",
        "    do_sample=False  # Deterministic output (temperature ignored when do_sample=False)\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "W5cqj19S3sk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Define embedding model**"
      ],
      "metadata": {
        "id": "zifjUTiN4IBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")"
      ],
      "metadata": {
        "id": "ofe5xLSc3vit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Document Retriever Class**"
      ],
      "metadata": {
        "id": "lSureCjJ4Njk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentRetriever:\n",
        "    def __init__(self, file_path: str):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.vectorstore = None\n",
        "        self.load_documents(file_path)\n",
        "\n",
        "    def load_documents(self, file_path: str):\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"Document not found at {file_path}\")\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        docs = loader.load()\n",
        "        # Use a smaller chunk size to avoid long input prompts\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\n",
        "        splits = text_splitter.split_documents(docs)\n",
        "        self.vectorstore = FAISS.from_documents(splits, self.embedding_model)\n",
        "\n",
        "    def retrieve(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        if not self.vectorstore:\n",
        "            raise ValueError(\"Vectorstore not initialized\")\n",
        "        question = state.get(\"question\", \"\")\n",
        "        relevant_docs = self.vectorstore.similarity_search(question, k=3)\n",
        "        return {\n",
        "            \"documents\": relevant_docs,\n",
        "            \"question\": question,\n",
        "            \"retry_count\": state.get(\"retry_count\", 0)\n",
        "        }"
      ],
      "metadata": {
        "id": "Soa0h4Kx3x78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Answer Generator Class**"
      ],
      "metadata": {
        "id": "V7CWwfwR4Rvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerGenerator:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def generate(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        context = \"\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
        "        # Truncate context to avoid exceeding model's max input length (512 tokens for Flan-T5-Large)\n",
        "        context = context[:1800]\n",
        "        prompt = f\"Based on the following context:\\n{context}\\n\\nQuestion: {state['question']}\\nAnswer:\"\n",
        "        try:\n",
        "            result = self.llm(prompt)\n",
        "            # HuggingFacePipeline returns a string or a list (depends on LangChain version)\n",
        "            if isinstance(result, str):\n",
        "                answer = result.strip()\n",
        "            elif isinstance(result, list) and len(result) > 0 and \"generated_text\" in result[0]:\n",
        "                answer = result[0][\"generated_text\"].strip()\n",
        "            else:\n",
        "                answer = str(result).strip()\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"documents\": state[\"documents\"],\n",
        "                \"question\": state[\"question\"],\n",
        "                \"verified\": False,\n",
        "                \"retry_count\": state.get(\"retry_count\", 0)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": \"I couldn't generate an answer due to an error.\",\n",
        "                \"documents\": state[\"documents\"],\n",
        "                \"question\": state[\"question\"],\n",
        "                \"verified\": False,\n",
        "                \"retry_count\": state.get(\"retry_count\", 0)\n",
        "            }"
      ],
      "metadata": {
        "id": "24KwEKej32Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Answer Verifier Class**"
      ],
      "metadata": {
        "id": "wdltL5FU4V39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerVerifier:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def verify(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        context = \"\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
        "        # Truncate context for verification as well\n",
        "        context = context[:1800]\n",
        "        verification_prompt = (\n",
        "            f\"Verify if this answer is correct based on the context:\\n\"\n",
        "            f\"Context: {context}\\n\"\n",
        "            f\"Question: {state['question']}\\n\"\n",
        "            f\"Answer: {state['answer']}\\n\"\n",
        "            f\"Respond only with 'True' or 'False':\"\n",
        "        )\n",
        "        try:\n",
        "            result = self.llm(verification_prompt)\n",
        "            if isinstance(result, str):\n",
        "                verification = result.strip().lower()\n",
        "            elif isinstance(result, list) and len(result) > 0 and \"generated_text\" in result[0]:\n",
        "                verification = result[0][\"generated_text\"].strip().lower()\n",
        "            else:\n",
        "                verification = str(result).strip().lower()\n",
        "            is_verified = \"true\" in verification\n",
        "            return {\n",
        "                \"verified\": is_verified,\n",
        "                \"answer\": state[\"answer\"],\n",
        "                \"documents\": state[\"documents\"],\n",
        "                \"question\": state[\"question\"],\n",
        "                \"retry_count\": state.get(\"retry_count\", 0) + 1\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error verifying answer: {str(e)}\")\n",
        "            return {\n",
        "                \"verified\": False,\n",
        "                \"answer\": state[\"answer\"],\n",
        "                \"documents\": state[\"documents\"],\n",
        "                \"question\": state[\"question\"],\n",
        "                \"retry_count\": state.get(\"retry_count\", 0) + 1\n",
        "            }"
      ],
      "metadata": {
        "id": "gyiCnaio35Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Workflow logic**"
      ],
      "metadata": {
        "id": "4XjEg_eJ4ZX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state: Dict[str, Any]) -> str:\n",
        "    if state[\"verified\"]:\n",
        "        return \"end\"\n",
        "    if state.get(\"retry_count\", 0) >= 2:\n",
        "        return \"end\"\n",
        "    return \"retry\"\n",
        "\n",
        "workflow = Graph()\n",
        "\n",
        "retriever = DocumentRetriever(\"computer_science_is_foundational.pdf\")  # Update path if needed\n",
        "generator = AnswerGenerator(llm)\n",
        "verifier = AnswerVerifier(llm)\n",
        "\n",
        "workflow.add_node(\"retriever\", retriever.retrieve)\n",
        "workflow.add_node(\"generator\", generator.generate)\n",
        "workflow.add_node(\"verifier\", verifier.verify)\n",
        "\n",
        "workflow.add_edge(\"retriever\", \"generator\")\n",
        "workflow.add_edge(\"generator\", \"verifier\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"verifier\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"end\": END,\n",
        "        \"retry\": \"retriever\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.set_entry_point(\"retriever\")\n",
        "agent = workflow.compile()"
      ],
      "metadata": {
        "id": "GPkyIItp39sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print(\"Document QA System - Type 'exit' to quit\")\n",
        "        while True:\n",
        "            question = input(\"\\nEnter your question: \").strip()\n",
        "            if question.lower() == 'exit':\n",
        "                break\n",
        "            if not question:\n",
        "                print(\"Please enter a valid question.\")\n",
        "                continue\n",
        "\n",
        "            result = agent.invoke({\n",
        "                \"question\": question,\n",
        "                \"retry_count\": 0\n",
        "            })\n",
        "\n",
        "            print(\"\\nAnswer:\", result[\"answer\"])\n",
        "            print(\"\\nSource references:\")\n",
        "            for i, doc in enumerate(result[\"documents\"], 1):\n",
        "                print(f\"{i}. {doc.page_content[:150]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")"
      ],
      "metadata": {
        "id": "UQpaNI6oB1QZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}