{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. INSTALL DEPENDENCIES**"
      ],
      "metadata": {
        "id": "Ky6xeTcaoi_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q transformers sentence-transformers deap datasets tqdm matplotlib"
      ],
      "metadata": {
        "id": "FAmBu1uWJBiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. IMPORTS & CONFIG**"
      ],
      "metadata": {
        "id": "cyOUFoJVosan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "from deap import base, creator, tools\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# For reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Genetic Algorithm Parameters\n",
        "POPULATION_SIZE = 20\n",
        "GENERATIONS = 10\n",
        "CX_PROB = 0.5\n",
        "MUT_PROB = 0.3\n",
        "ELITE_SIZE = 2\n",
        "\n",
        "# Model selection\n",
        "MODEL_NAME = \"gpt2\"\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\""
      ],
      "metadata": {
        "id": "Vuf1JaowJI41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. DATASET LOADING**"
      ],
      "metadata": {
        "id": "_Tt1rpGqo72Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_fallback_dataset():\n",
        "    articles = [\n",
        "        \"The quick brown fox jumps over the lazy dog. This classic sentence contains all letters in the English alphabet.\",\n",
        "        \"Global temperatures continue to rise, with scientists predicting a 1.5Â°C increase within the next decade.\",\n",
        "        \"Artificial intelligence is transforming industries from healthcare to finance, with new applications emerging daily.\",\n",
        "        \"Researchers have discovered a potential breakthrough in battery technology that could double electric vehicle range.\"\n",
        "    ]\n",
        "    summaries = [\n",
        "        \"The fox sentence demonstrates all English letters.\",\n",
        "        \"Climate scientists warn of significant temperature increases.\",\n",
        "        \"AI is revolutionizing multiple industry sectors.\",\n",
        "        \"New battery tech may greatly improve EV performance.\"\n",
        "    ]\n",
        "    return [{'article': a, 'highlights': s} for a, s in zip(articles, summaries)]\n",
        "\n",
        "def load_data():\n",
        "    try:\n",
        "        dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:20]\", trust_remote_code=True)\n",
        "        logging.info(\"Loaded CNN/DailyMail dataset successfully.\")\n",
        "        reference_summaries = [item['highlights'] for item in dataset]\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error loading CNN/DailyMail: {e}\")\n",
        "        dataset = create_fallback_dataset()\n",
        "        reference_summaries = [item['highlights'] for item in dataset]\n",
        "        logging.info(\"Using fallback dataset.\")\n",
        "    return dataset, reference_summaries\n",
        "\n",
        "dataset, reference_summaries = load_data()"
      ],
      "metadata": {
        "id": "CxUmWaOxo7mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. MODEL LOADING**"
      ],
      "metadata": {
        "id": "aAhT5lIypEJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=MODEL_NAME,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    max_new_tokens=80,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "eval_model = SentenceTransformer(EMBED_MODEL_NAME, device='cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "DVQB3c3Vo7kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. PROMPT POOLS**"
      ],
      "metadata": {
        "id": "JBx8eJbepIe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INSTRUCTION_POOL = [\n",
        "    \"Summarize this text\",\n",
        "    \"Create a summary of\",\n",
        "    \"Briefly explain\",\n",
        "    \"Generate a short summary of\",\n",
        "    \"Condense this text\"\n",
        "]\n",
        "\n",
        "STYLE_POOL = [\n",
        "    \"in a professional tone\",\n",
        "    \"in simple language\",\n",
        "    \"using bullet points\",\n",
        "    \"in 3 sentences maximum\",\n",
        "    \"focusing on main ideas\"\n",
        "]\n",
        "\n",
        "def create_individual():\n",
        "    return [\n",
        "        random.choice(INSTRUCTION_POOL),\n",
        "        random.choice(STYLE_POOL)\n",
        "    ]"
      ],
      "metadata": {
        "id": "aQgksURjo7hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. GENETIC OPERATORS**"
      ],
      "metadata": {
        "id": "hkHPBDIRpRiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mutate_individual(individual, indpb):\n",
        "    for i in range(len(individual)):\n",
        "        if random.random() < indpb:\n",
        "            if i == 0:\n",
        "                individual[i] = random.choice(INSTRUCTION_POOL)\n",
        "            else:\n",
        "                individual[i] = random.choice(STYLE_POOL)\n",
        "    return (individual,)\n",
        "\n",
        "def evaluate(individual, k=3):\n",
        "    \"\"\"\n",
        "    Evaluate prompt on k random samples for robustness.\n",
        "    Returns tuple with average cosine similarity.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        indices = np.random.choice(len(dataset), size=k, replace=False)\n",
        "        similarities = []\n",
        "        for idx in indices:\n",
        "            text_sample = dataset[idx]['article'] if isinstance(dataset[idx], dict) else dataset[idx]['article']\n",
        "            reference = reference_summaries[idx]\n",
        "            prompt = f'{individual[0]}: \"{text_sample}\" {individual[1]}'\n",
        "            output = generator(prompt, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
        "            generated_text = output[len(prompt):].strip()\n",
        "            emb_gen = eval_model.encode(generated_text)\n",
        "            emb_ref = eval_model.encode(reference)\n",
        "            similarity = np.dot(emb_gen, emb_ref) / (np.linalg.norm(emb_gen) * np.linalg.norm(emb_ref))\n",
        "            similarities.append(similarity)\n",
        "        avg_sim = float(np.mean(similarities))\n",
        "        return (avg_sim,)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error in evaluation: {str(e)[:120]}...\")\n",
        "        return (0.0,)"
      ],
      "metadata": {
        "id": "hFEeiRRno7eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. DEAP SETUP**"
      ],
      "metadata": {
        "id": "DJ6wiiuXpYCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", mutate_individual, indpb=0.3)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "class ScalarStats(tools.Statistics):\n",
        "    def __init__(self, key):\n",
        "        super().__init__(key)\n",
        "    def compile(self, data):\n",
        "        values = [self.key(ind) for ind in data]\n",
        "        if values and isinstance(values[0], tuple):\n",
        "            values = [v[0] for v in values]\n",
        "        return {\n",
        "            \"avg\": np.mean(values),\n",
        "            \"min\": np.min(values),\n",
        "            \"max\": np.max(values)\n",
        "        }"
      ],
      "metadata": {
        "id": "jbFoqhq8o7bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. GA MAIN LOOP**"
      ],
      "metadata": {
        "id": "FpEqREEzpe3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    pop = toolbox.population(n=POPULATION_SIZE)\n",
        "    hof = tools.HallOfFame(5)\n",
        "    stats = ScalarStats(lambda ind: ind.fitness.values)\n",
        "    logbook = tools.Logbook()\n",
        "    logbook.header = [\"gen\", \"nevals\", \"avg\", \"min\", \"max\"]\n",
        "\n",
        "    # Evaluate initial population\n",
        "    logging.info(\"Evaluating initial population...\")\n",
        "    fitnesses = list(tqdm(map(toolbox.evaluate, pop), total=len(pop)))\n",
        "    for ind, fit in zip(pop, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    hof.update(pop)\n",
        "    record = stats.compile(pop)\n",
        "    logbook.record(gen=0, nevals=len(pop), **record)\n",
        "    logging.info(f\"Generation 0: Max fitness = {record['max']:.4f}\")\n",
        "\n",
        "    # Evolution loop\n",
        "    for gen in range(1, GENERATIONS + 1):\n",
        "        offspring = toolbox.select(pop, len(pop) - ELITE_SIZE)\n",
        "        offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "        # Crossover\n",
        "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "            if random.random() < CX_PROB:\n",
        "                toolbox.mate(child1, child2)\n",
        "                del child1.fitness.values\n",
        "                del child2.fitness.values\n",
        "\n",
        "        # Mutation\n",
        "        for mutant in offspring:\n",
        "            if random.random() < MUT_PROB:\n",
        "                toolbox.mutate(mutant)\n",
        "                del mutant.fitness.values\n",
        "\n",
        "        # Evaluate new individuals\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = list(tqdm(map(toolbox.evaluate, invalid_ind), total=len(invalid_ind)))\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        # Elitism\n",
        "        elite = sorted(pop, key=lambda ind: ind.fitness.values[0], reverse=True)[:ELITE_SIZE]\n",
        "        pop[:] = elite + offspring\n",
        "        hof.update(pop)\n",
        "\n",
        "        # Logging\n",
        "        record = stats.compile(pop)\n",
        "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
        "        logging.info(f\"Generation {gen}: Max fitness = {record['max']:.4f}\")\n",
        "\n",
        "    return pop, hof, logbook"
      ],
      "metadata": {
        "id": "GTUJuFk_pcMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. RESULTS & VISUALIZATION**"
      ],
      "metadata": {
        "id": "kVAdiYIkpmQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(log):\n",
        "    gen = log.select(\"gen\")\n",
        "    avg = log.select(\"avg\")\n",
        "    max_ = log.select(\"max\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(gen, avg, label=\"Average Fitness\")\n",
        "    plt.plot(gen, max_, label=\"Max Fitness\")\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Fitness (Cosine Similarity)\")\n",
        "    plt.title(\"Prompt Optimization Progress\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def display_top_prompts(hall_of_fame, dataset, generator):\n",
        "    print(\"\\nTop 5 Optimized Prompts:\")\n",
        "    for i, ind in enumerate(hall_of_fame):\n",
        "        prompt = f'{ind[0]}: \"[TEXT]\" {ind[1]}'\n",
        "        print(f\"\\nRank {i+1} (Fitness: {ind.fitness.values[0]:.4f}):\")\n",
        "        print(prompt)\n",
        "        sample_text = dataset[i % len(dataset)]['article'] if isinstance(dataset[i % len(dataset)], dict) else dataset[i % len(dataset)]['article']\n",
        "        full_prompt = f'{ind[0]}: \"{sample_text}\" {ind[1]}'\n",
        "        output = generator(full_prompt, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
        "        print(\"\\nExample Output:\")\n",
        "        print(output[len(full_prompt):].strip())\n",
        "\n",
        "def calculate_metrics(prompt, text, reference):\n",
        "    full_prompt = f'{prompt[0]}: \"{text}\" {prompt[1]}'\n",
        "    output = generator(full_prompt, num_return_sequences=1, truncation=True)[0]['generated_text']\n",
        "    generated_text = output[len(full_prompt):].strip()\n",
        "    emb_gen = eval_model.encode(generated_text)\n",
        "    emb_ref = eval_model.encode(reference)\n",
        "    similarity = np.dot(emb_gen, emb_ref) / (np.linalg.norm(emb_gen) * np.linalg.norm(emb_ref))\n",
        "    word_count = len(generated_text.split())\n",
        "    unique_words = len(set(generated_text.lower().split()))\n",
        "    diversity = unique_words / word_count if word_count > 0 else 0\n",
        "    return similarity, word_count, diversity, generated_text\n",
        "\n",
        "def quantitative_comparison(hall_of_fame, dataset, reference_summaries):\n",
        "    print(\"\\nQuantitative Comparison:\")\n",
        "    print(\"=\"*50)\n",
        "    initial_prompt = create_individual()\n",
        "    optimized_prompt = hall_of_fame[0]\n",
        "    sample = dataset[0]\n",
        "    ref = reference_summaries[0]\n",
        "    sample_text = sample['article'] if isinstance(sample, dict) else sample['article']\n",
        "\n",
        "    init_sim, init_words, init_div, init_gen = calculate_metrics(initial_prompt, sample_text, ref)\n",
        "    opt_sim, opt_words, opt_div, opt_gen = calculate_metrics(optimized_prompt, sample_text, ref)\n",
        "\n",
        "    print(f\"\\nInitial Prompt: {initial_prompt[0]}: \\\"[TEXT]\\\" {initial_prompt[1]}\")\n",
        "    print(f\"Generated: {init_gen}\")\n",
        "    print(f\"Similarity: {init_sim:.4f} | Word Count: {init_words} | Diversity: {init_div:.4f}\")\n",
        "\n",
        "    print(f\"\\nOptimized Prompt: {optimized_prompt[0]}: \\\"[TEXT]\\\" {optimized_prompt[1]}\")\n",
        "    print(f\"Generated: {opt_gen}\")\n",
        "    print(f\"Similarity: {opt_sim:.4f} | Word Count: {opt_words} | Diversity: {opt_div:.4f}\")\n",
        "\n",
        "    improvement = (opt_sim - init_sim) / init_sim * 100 if init_sim != 0 else float('inf')\n",
        "    print(f\"\\nImprovement: {improvement:.2f}% increase in similarity to reference\")"
      ],
      "metadata": {
        "id": "fQwa2RTtpcJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. RUN EVERYTHING**"
      ],
      "metadata": {
        "id": "BstgNf86ptqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    final_pop, hall_of_fame, stats_log = main()\n",
        "    plot_results(stats_log)\n",
        "    display_top_prompts(hall_of_fame, dataset, generator)\n",
        "    quantitative_comparison(hall_of_fame, dataset, reference_summaries)"
      ],
      "metadata": {
        "id": "20_Z7XI5pcGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}