{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9JVwZlhOHrj"
      },
      "source": [
        "# **Install required packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mw16S_O1OCW6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wikipedia langchain langchain-community langchain-huggingface chromadb sentence-transformers transformers torch accelerate protobuf==4.25.3 google-api-core==2.19.0 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSm0MGzoOOaK"
      },
      "source": [
        "# **Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HxqKVyKOOMVa"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UEyEGf_OVdM"
      },
      "source": [
        "# **Configure Wikipedia to use Farsi language**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fi-4GsDlOTED"
      },
      "outputs": [],
      "source": [
        "wikipedia.set_lang('fa')  # Set Wikipedia to Persian (Farsi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJhwyF4GOcXi"
      },
      "source": [
        "# **Login to Hugging Face for accessing the Gemma model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S6Tu0jDdOaGJ",
        "outputId": "ca20076c-80a9-41b2-950c-b4a2cd386397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Successfully logged in to Hugging Face!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    token = getpass(\"Enter your Hugging Face token (press Enter to skip): \")\n",
        "    if token.strip():\n",
        "        login(token=token)\n",
        "        print(\"✅ Successfully logged in to Hugging Face!\")\n",
        "except:\n",
        "    print(\"➡️ Proceeding without token...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Vp_IyKOja7"
      },
      "source": [
        "# **Load the Gemma 2B model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T8b0c7WAOgjC",
        "outputId": "66b9fb49-e3f3-4693-fae1-6fba82dea784"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bbc5118e931434885ad5bd937620426",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45e7727934fb45729c3a1378f3896be9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ab2eb4727b047cea6b025ebbb31dc35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a96f58e29af4275ab1253b640948583",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8be6ceca4e242c8a9dfaa54959946af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "689fb8cab3554648bc2099c3d0fb91cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c04f016a50d40639779a83af7a33175",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd580771a09042ffb5a14f3f8ba2f2d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f9edd7208924fc4b6291720696fcae4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf789bb03a3e427da4f97430fcf7ef85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e508cbcad1b41759b02a57efda9c795",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"google/gemma-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,       # Use float16 for memory efficiency\n",
        "    device_map=\"cpu\",                # Use CPU to avoid CUDA issues (set to \"auto\" for GPU)\n",
        "    trust_remote_code=True,          # Allow custom model code\n",
        "    low_cpu_mem_usage=True           # Optimize memory usage\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7CD3akwOrAa"
      },
      "source": [
        "# **Create a text-generation pipeline for LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WKNAe-qiOotx",
        "outputId": "6316e6f3-0845-4cd8-c294-7e8e4d32c7ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=150,              # Limit response length\n",
        "    temperature=0.3,                 # Low temperature for less randomness\n",
        "    top_p=0.9,                       # Top-p sampling for focused outputs\n",
        "    return_full_text=False           # Return only generated text\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdjQmVmiOyqi"
      },
      "source": [
        "# **Define a prompt template for RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rnfXV_DROwYC"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Based on the following information, provide an answer:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4FOAg6uPDJh"
      },
      "source": [
        "# **RAG function to retrieve context from Wikipedia and generate answers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wbMI5RUmO_VZ"
      },
      "outputs": [],
      "source": [
        "def rag_with_wikipedia(question, max_chars=2000):\n",
        "    \"\"\"\n",
        "    Retrieves relevant Wikipedia article content and generates an answer using the Gemma model.\n",
        "\n",
        "    Args:\n",
        "        question (str): The query to search on Wikipedia.\n",
        "        max_chars (int): Maximum length of the context to retrieve (default: 2000).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (response, context, page_title) - Generated answer, retrieved context, and Wikipedia article title.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Search Wikipedia for relevant articles\n",
        "        search_results = wikipedia.search(question)\n",
        "        if not search_results:\n",
        "            return \"No relevant Wikipedia article found in Persian.\", \"\", \"\"\n",
        "\n",
        "        # Step 2: Load the first relevant article\n",
        "        page_title = search_results[0]  # e.g., \"پایتون (زبان برنامه‌نویسی)\"\n",
        "        page = wikipedia.page(page_title)\n",
        "\n",
        "        # Step 3: Extract content (truncate to max_chars for efficiency)\n",
        "        full_content = page.content\n",
        "        context = full_content[:max_chars] + \"...\" if len(full_content) > max_chars else full_content\n",
        "\n",
        "        # Step 4: Generate response using the language model\n",
        "        response = llm.invoke(prompt.format(context=context, question=question))\n",
        "        response = response.split(\"پاسخ:\")[-1].strip()  # Extract only the answer part\n",
        "\n",
        "        return response, context, page_title\n",
        "    except Exception as e:\n",
        "        return f\"Error accessing Wikipedia: {str(e)}\", \"\", \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMdsWAE5PNcL"
      },
      "source": [
        "# **Test the RAG system with sample questions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aiCFWyKN5WC"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"پایتون چیست؟\",             # What is Python?\n",
        "    \"چرا پایتون محبوب است؟\",   # Why is Python popular?\n",
        "    \"پایتون کجا استفاده می‌شود؟\" # Where is Python used?\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    response, context, title = rag_with_wikipedia(query)\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Wikipedia Article: {title}\")\n",
        "    print(f\"Context (Summary): {context[:200]}...\")  # Show first 200 chars of context\n",
        "    print(f\"Answer: {response}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}