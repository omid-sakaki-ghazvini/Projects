{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOq3jWuiW5HnyMYT1jkz9KG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1-Install required packages**"
      ],
      "metadata": {
        "id": "k3WiZw9kvPVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q torch torchvision pillow faiss-cpu umap-learn matplotlib tqdm requests\n",
        "!pip install -q git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "o7eAwWCl2Akq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import umap.umap_ as umap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import normalize\n",
        "import faiss\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Ip2okBss2GWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2-Create necessary directories**"
      ],
      "metadata": {
        "id": "yFtUsB_U2Ibz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/images', exist_ok=True)\n",
        "os.makedirs('/content/output', exist_ok=True)"
      ],
      "metadata": {
        "id": "MwldyAsx2NDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3-Download alternative flower dataset (from a reliable TensorFlow source)**"
      ],
      "metadata": {
        "id": "d8HUW6af2PFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "!wget -q --show-progress {dataset_url} -O /content/flower_dataset.tgz\n",
        "!tar -xzf /content/flower_dataset.tgz -C /content/images/"
      ],
      "metadata": {
        "id": "2t4ONzcL2UtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4-Randomly select 200 images**"
      ],
      "metadata": {
        "id": "U8pAUIDm2bGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_files = []\n",
        "for root, _, files in os.walk('/content/images/flower_photos'):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            all_files.append(os.path.join(root, file))\n",
        "\n",
        "selected_files = random.sample(all_files, min(200, len(all_files)))\n",
        "\n",
        "# Clean up\n",
        "!rm /content/flower_dataset.tgz\n",
        "print(f\"\\nNumber of images ready for processing: {len(selected_files)}\")"
      ],
      "metadata": {
        "id": "SuTgVJQU2epb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "YKu3eTea2nXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5-Load CLIP model**"
      ],
      "metadata": {
        "id": "xFxOp5IB2z3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "print(\"CLIP model loaded successfully.\")"
      ],
      "metadata": {
        "id": "j5XwHO0n24xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6-Extract embeddings**"
      ],
      "metadata": {
        "id": "UkgcsKc22qX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embeddings(image_paths, batch_size=32):\n",
        "    embeddings = []\n",
        "    valid_paths = []\n",
        "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Processing images\", unit=\"batch\"):\n",
        "        batch_paths = image_paths[i:i+batch_size]\n",
        "        batch_images = []\n",
        "        for path in batch_paths:\n",
        "            try:\n",
        "                image = Image.open(path).convert(\"RGB\")\n",
        "                batch_images.append(preprocess(image))\n",
        "                valid_paths.append(path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {path}: {str(e)}\")\n",
        "        if batch_images:\n",
        "            image_input = torch.stack(batch_images).to(device)\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = model.encode_image(image_input).cpu().numpy()\n",
        "            embeddings.append(batch_embeddings)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    embeddings = normalize(embeddings, axis=1)\n",
        "    return embeddings, valid_paths\n",
        "\n",
        "embeddings, valid_paths = extract_embeddings(selected_files)\n",
        "print(f\"Extracted embeddings: {embeddings.shape}\")"
      ],
      "metadata": {
        "id": "6M8sqzpp2tL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7-Dimensionality reduction with UMAP**"
      ],
      "metadata": {
        "id": "9oEqOCnB3A9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reducer = umap.UMAP(\n",
        "    n_components=2,\n",
        "    metric='cosine',\n",
        "    random_state=42,\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"Reducing dimensions with UMAP...\")\n",
        "embeddings_2d = reducer.fit_transform(embeddings)\n",
        "print(\"UMAP dimensionality reduction complete.\")"
      ],
      "metadata": {
        "id": "oMOg_FTz3DFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8-Show advanced visualization with images on scatter**"
      ],
      "metadata": {
        "id": "LF2MM0DL3II6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 16), dpi=120)\n",
        "ax = plt.gca()\n",
        "ax.set_facecolor('#f5f5f5')\n",
        "plt.grid(color='white', linestyle='--', linewidth=0.5)\n",
        "\n",
        "for i, (x, y) in enumerate(embeddings_2d):\n",
        "    try:\n",
        "        img = Image.open(valid_paths[i])\n",
        "        img.thumbnail((80, 80), Image.LANCZOS)\n",
        "        ax.imshow(img, extent=(x-3, x+3, y-3, y+3), alpha=0.9)\n",
        "        plt.text(x, y+4, os.path.basename(valid_paths[i]), fontsize=8, ha='center', alpha=0.7)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "plt.title('Semantic Map of Flower Images (UMAP + CLIP)', fontsize=16, pad=20)\n",
        "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
        "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "def visualize_search_results(query_path, results):\n",
        "    plt.figure(figsize=(18, 4))\n",
        "    plt.subplot(1, 6, 1)\n",
        "    query_img = Image.open(query_path)\n",
        "    plt.imshow(query_img)\n",
        "    plt.title('Query Image', fontsize=10)\n",
        "    plt.axis('off')\n",
        "    for i, result in enumerate(results[:5]):\n",
        "        plt.subplot(1, 6, i+2)\n",
        "        img = Image.open(result)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f'Result {i+1}\\n{os.path.basename(result)}', fontsize=8)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZjD-FLVa3OmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9-Build FAISS index for search**"
      ],
      "metadata": {
        "id": "vwgAmsuu3YVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_faiss = embeddings.astype('float32')\n",
        "dimension = embeddings_faiss.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "faiss.normalize_L2(embeddings_faiss)\n",
        "index.add(embeddings_faiss)"
      ],
      "metadata": {
        "id": "zBoD2JMn3fHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_path = random.choice(valid_paths)\n",
        "query_image = Image.open(query_path)\n",
        "query_input = preprocess(query_image).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    query_embedding = model.encode_image(query_input).cpu().numpy()\n",
        "query_embedding = normalize(query_embedding.reshape(1, -1)).astype('float32')\n",
        "faiss.normalize_L2(query_embedding)\n",
        "distances, indices = index.search(query_embedding, 5)\n",
        "results = [valid_paths[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "K9-dMBKS3h66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show search results\n",
        "visualize_search_results(query_path, results)"
      ],
      "metadata": {
        "id": "GO56rwl33ocD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10-Density visualization**"
      ],
      "metadata": {
        "id": "MczWrglO3tUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
        "           c=np.linalg.norm(embeddings, axis=1), cmap='viridis', alpha=0.7, s=100)\n",
        "plt.colorbar(label='Embedding Norm')\n",
        "for i in np.random.choice(len(valid_paths), min(20, len(valid_paths)), replace=False):\n",
        "    plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1],\n",
        "            os.path.basename(valid_paths[i]), fontsize=8, alpha=0.8)\n",
        "plt.title('Density of Image Embeddings', fontsize=16)\n",
        "plt.xlabel('UMAP Dimension 1')\n",
        "plt.ylabel('UMAP Dimension 2')\n",
        "plt.grid(True, alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JMZP10gI1Mji"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}