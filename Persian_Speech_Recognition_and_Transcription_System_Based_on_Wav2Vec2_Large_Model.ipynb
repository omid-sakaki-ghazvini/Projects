{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 1: INSTALLATION\n",
        "# ===========================================================\n",
        "# Install required libraries with GPU support and SafeTensors compatibility\n",
        "!pip install -q torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q \"datasets==2.16.1\" librosa==0.10.2.post1\n",
        "!pip install -q transformers jiwer accelerate soundfile safetensors\n",
        "!pip install -qU gradio==4.44.0"
      ],
      "metadata": {
        "id": "OKnrcyT8RDmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 2: ENVIRONMENT SETUP\n",
        "# ===========================================================\n",
        "# Reduce GPU memory fragmentation (highly recommended for T4 GPU)\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "SelAFFbzRFMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 3: IMPORT LIBRARIES\n",
        "# ===========================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Union\n",
        "import gradio as gr\n",
        "from huggingface_hub import login\n",
        "from jiwer import wer, cer"
      ],
      "metadata": {
        "id": "8Y-9B7KSRLXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 4: AUTHENTICATION\n",
        "# ===========================================================\n",
        "# Log in to Hugging Face Hub\n",
        "HF_TOKEN = \"Your token\"  # Your token\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "Lhrq5S0SRNl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 5: LOAD DATASET\n",
        "# ===========================================================\n",
        "# Load the Persian speech dataset and resample to 16kHz\n",
        "dataset = load_dataset(\"SeyedAli/Persian-Speech-Dataset\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "dCTS5zeyRPgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 6: CONFIGURATION\n",
        "# ===========================================================\n",
        "# Model and repository configuration\n",
        "BASE_MODEL = \"jonatasgrosman/wav2vec2-large-xlsr-53-persian\"\n",
        "HF_USERNAME = \"Your USERNAME\"\n",
        "MODEL_REPO_NAME = f\"{HF_USERNAME}/wav2vec2-large-xlsr53-fa-finetuned-gpu-final\""
      ],
      "metadata": {
        "id": "peiD_OGoRSKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 7: LOAD PROCESSOR\n",
        "# ===========================================================\n",
        "# Load the processor with SafeTensors for secure loading\n",
        "processor = Wav2Vec2Processor.from_pretrained(BASE_MODEL, use_safetensors=True)"
      ],
      "metadata": {
        "id": "UTIMBXhERUi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 8: DATA PREPROCESSING\n",
        "# ===========================================================\n",
        "# Preprocess the data with normalization and length truncation to save memory\n",
        "MAX_LENGTH = 160000  # ~10 seconds at 16kHz\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    audio_array = np.asarray(audio[\"array\"], dtype=np.float32)\n",
        "\n",
        "    # Truncate long audios to prevent memory issues\n",
        "    if len(audio_array) > MAX_LENGTH:\n",
        "        audio_array = audio_array[:MAX_LENGTH]\n",
        "\n",
        "    # Normalize audio\n",
        "    max_abs = np.max(np.abs(audio_array))\n",
        "    if max_abs > 0:\n",
        "        audio_array = audio_array / max_abs\n",
        "\n",
        "    # Process input features\n",
        "    inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    batch[\"input_values\"] = inputs.input_values[0]\n",
        "\n",
        "    # Process labels\n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"transcript\"]).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "# Apply preprocessing\n",
        "dataset = dataset.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=[\"audio\", \"speaker_id\", \"gender\", \"ipa\", \"emotion\", \"transcript\"]\n",
        ")"
      ],
      "metadata": {
        "id": "eNT5d7gWRYfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 9: TRAIN-TEST SPLIT\n",
        "# ===========================================================\n",
        "# Split the dataset into train and evaluation sets\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "-pfG0sKaRa_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 10: DATA COLLATOR\n",
        "# ===========================================================\n",
        "# Custom data collator for CTC loss\n",
        "@dataclass\n",
        "class DataCollatorCTC:\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: bool = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n",
        "\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(label_features, padding=self.padding, return_tensors=\"pt\")\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorCTC(processor=processor)"
      ],
      "metadata": {
        "id": "j9-lktxhRd5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 11: LOAD MODEL\n",
        "# ===========================================================\n",
        "# Load the pre-trained model with SafeTensors\n",
        "print(\"Loading model with SafeTensors...\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer),\n",
        "    use_safetensors=True,\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.ctc_zero_infinity = True"
      ],
      "metadata": {
        "id": "xVVhU9reRiRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 12: METRICS\n",
        "# ===========================================================\n",
        "# Define WER and CER metrics using jiwer\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    return {\"wer\": wer(label_str, pred_str), \"cer\": cer(label_str, pred_str)}"
      ],
      "metadata": {
        "id": "icjVWUNzRk8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 13: TRAINING ARGUMENTS\n",
        "# ===========================================================\n",
        "# Optimized training arguments to prevent OOM on T4 GPU\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./wav2vec2-fa-gpu\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=400,\n",
        "    save_total_limit=3,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=10,\n",
        "    warmup_steps=300,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=MODEL_REPO_NAME,\n",
        "    hub_strategy=\"checkpoint\",\n",
        "    hub_private_repo=False,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=2,\n",
        "    optim=\"adamw_torch\",\n",
        "    save_safetensors=True,\n",
        ")"
      ],
      "metadata": {
        "id": "_hPHjB6_RnxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 14: TRAINER SETUP\n",
        "# ===========================================================\n",
        "# Create the Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        ")"
      ],
      "metadata": {
        "id": "75VvUUgmRqGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 15: START TRAINING\n",
        "# ===========================================================\n",
        "# Clear GPU cache and start training\n",
        "torch.cuda.empty_cache()\n",
        "print(\"üöÄ Starting final training without OOM on GPU...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZieEFsxxRxM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 16: SAVE AND PUSH TO HUB\n",
        "# ===========================================================\n",
        "# Save the model and processor, then push to Hugging Face Hub\n",
        "trainer.save_model()\n",
        "processor.save_pretrained(\"./wav2vec2-fa-gpu\")\n",
        "trainer.push_to_hub(\n",
        "    commit_message=\"Final fine-tune of Wav2Vec2-large-xlsr-53-persian with memory-optimized settings and SafeTensors\",\n",
        "    tags=[\"automatic-speech-recognition\", \"persian\", \"farsi\", \"wav2vec2\", \"safetensors\"]\n",
        ")"
      ],
      "metadata": {
        "id": "U_43VxkFR1zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 17: FINAL EVALUATION\n",
        "# ===========================================================\n",
        "# Evaluate the fine-tuned model\n",
        "metrics = trainer.evaluate()\n",
        "print(f\"\\n‚úÖ Final WER: {metrics['eval_wer']:.2%}\")\n",
        "print(f\"‚úÖ Final CER: {metrics['eval_cer']:.2%}\")"
      ],
      "metadata": {
        "id": "8H4ZZQNFR4Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 18: INFERENCE FUNCTION\n",
        "# ===========================================================\n",
        "# Function to transcribe audio files\n",
        "def transcribe_audio(audio_path):\n",
        "    speech, _ = librosa.load(audio_path, sr=16000)\n",
        "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "    return transcription"
      ],
      "metadata": {
        "id": "HnIcklxjR6Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 19: TEST ON SAMPLE\n",
        "# ===========================================================\n",
        "# Test the model on a sample from the evaluation set\n",
        "example = eval_dataset[0]\n",
        "\n",
        "original_text = processor.decode(example[\"labels\"], skip_special_tokens=True)\n",
        "\n",
        "input_values = example[\"input_values\"]\n",
        "if isinstance(input_values, list):\n",
        "    input_values = np.array(input_values)\n",
        "else:\n",
        "    input_values = input_values.numpy()\n",
        "\n",
        "sf.write(\"test_sample.wav\", input_values, 16000)\n",
        "\n",
        "result = transcribe_audio(\"test_sample.wav\")\n",
        "\n",
        "print(f\"\\nüìù ÿ™ÿ≥ÿ™ ŸÜŸÖŸàŸÜŸá:\")\n",
        "print(f\"ŸÖÿ™ŸÜ ÿßÿµŸÑ€å: {original_text}\")\n",
        "print(f\"ÿ™ÿ¥ÿÆ€åÿµ ŸÖÿØŸÑ: {result}\")"
      ],
      "metadata": {
        "id": "R_YwWekSR8KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 20: COMPLETION MESSAGE\n",
        "# ===========================================================\n",
        "print(f\"\\n‚úÖ Training completed successfully!\")\n",
        "print(f\"üîó Model uploaded to: https://huggingface.co/{MODEL_REPO_NAME}\")"
      ],
      "metadata": {
        "id": "x23SMHqBzyUS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}