{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1328792,"sourceType":"datasetVersion","datasetId":771078}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T16:21:59.564508Z","iopub.execute_input":"2025-04-21T16:21:59.564825Z","iopub.status.idle":"2025-04-21T16:22:17.476198Z","shell.execute_reply.started":"2025-04-21T16:21:59.564799Z","shell.execute_reply":"2025-04-21T16:22:17.475568Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport spacy\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Enable deterministic CuDNN for consistent behavior\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Set device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load English language model for tokenization\nspacy_eng = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:05:25.932250Z","iopub.execute_input":"2025-04-21T17:05:25.932542Z","iopub.status.idle":"2025-04-21T17:05:26.511647Z","shell.execute_reply.started":"2025-04-21T17:05:25.932517Z","shell.execute_reply":"2025-04-21T17:05:26.511090Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# **1- Vocabulary & Text Processing**","metadata":{}},{"cell_type":"code","source":"class Vocabulary:\n    \"\"\"Vocabulary class to handle text tokenization and numericalization\"\"\"\n    \n    def __init__(self, freq_threshold):\n        # Initialize special tokens\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n    \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer_eng(text):\n        \"\"\"Tokenize English text using spaCy\"\"\"\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n    \n    def build_vocabulary(self, sentences):\n        \"\"\"Build vocabulary from sentences based on frequency threshold\"\"\"\n        idx = 4  # Start index after special tokens\n        frequency = {}\n        \n        for sentence in sentences:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequency:\n                    frequency[word] = 1\n                else:\n                    frequency[word] += 1\n                \n                # Add word to vocabulary if it reaches frequency threshold\n                if frequency[word] == self.freq_threshold:\n                    self.itos[idx] = word\n                    self.stoi[word] = idx\n                    idx += 1\n    \n    def numericalize(self, sentence):\n        \"\"\"Convert sentence to numerical tokens\"\"\"\n        tokenized_text = self.tokenizer_eng(sentence)\n        return [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] \n                for word in tokenized_text]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:05:53.251350Z","iopub.execute_input":"2025-04-21T17:05:53.251637Z","iopub.status.idle":"2025-04-21T17:05:53.258367Z","shell.execute_reply.started":"2025-04-21T17:05:53.251615Z","shell.execute_reply":"2025-04-21T17:05:53.257625Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# **2- Dataset Preparation**","metadata":{}},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    \"\"\"Dataset class for Flickr8k images and captions\"\"\"\n    \n    def __init__(self, root_dir, caption_path, freq_threshold=5, transform=None):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(caption_path)\n        self.transform = transform\n        self.captions = self.df['caption']\n        self.images = self.df['image']\n        \n        # Initialize and build vocabulary\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions.tolist())\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        \"\"\"Return image and numericalized caption for a given index\"\"\"\n        caption = self.captions[index]\n        img_name = self.images[index]\n        img = Image.open(os.path.join(self.root_dir, img_name)).convert(\"RGB\")\n        \n        # Apply image transformations if specified\n        if self.transform:\n            img = self.transform(img)\n        \n        # Convert caption to numerical tokens with SOS and EOS\n        numerical_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numerical_caption += self.vocab.numericalize(caption)\n        numerical_caption.append(self.vocab.stoi[\"<EOS>\"])\n        \n        return img, torch.tensor(numerical_caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyCollate:\n    \"\"\"Collate function to handle padding in variable-length captions\"\"\"\n    \n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n    \n    def __call__(self, batch):\n        # Stack images\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        \n        # Pad captions and transpose for LSTM (seq_len, batch_size)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n        \n        return imgs, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:06:25.439551Z","iopub.execute_input":"2025-04-21T17:06:25.439841Z","iopub.status.idle":"2025-04-21T17:06:25.444761Z","shell.execute_reply.started":"2025-04-21T17:06:25.439819Z","shell.execute_reply":"2025-04-21T17:06:25.444108Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# **3- CNN Encoder Architecture**","metadata":{}},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    \"\"\"CNN encoder using ResNet-50 pretrained model\"\"\"\n    \n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        # Load pretrained ResNet-50\n        self.cnn = models.resnet50(pretrained=True)\n        \n        # Freeze all parameters\n        for param in self.cnn.parameters():\n            param.requires_grad_(False)\n        \n        # Replace final fully connected layer\n        in_features = self.cnn.fc.in_features\n        self.cnn.fc = nn.Linear(in_features, embed_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, images):\n        \"\"\"Extract image features\"\"\"\n        features = self.dropout(self.relu(self.cnn(images)))\n        return features.unsqueeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:06:54.265242Z","iopub.execute_input":"2025-04-21T17:06:54.265514Z","iopub.status.idle":"2025-04-21T17:06:54.270755Z","shell.execute_reply.started":"2025-04-21T17:06:54.265495Z","shell.execute_reply":"2025-04-21T17:06:54.270084Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# **4- Attention Mechanism**","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    \"\"\"Attention mechanism for decoder\"\"\"\n    \n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        self.full_att = nn.Linear(attention_dim, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n    \n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"Calculate attention weights and context vector\"\"\"\n        att1 = self.encoder_att(encoder_out)\n        att2 = self.decoder_att(decoder_hidden)\n        \n        # Compute attention scores\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n        alpha = self.softmax(att)\n        \n        # Compute context vector\n        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n        \n        return context, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:07:07.122818Z","iopub.execute_input":"2025-04-21T17:07:07.123059Z","iopub.status.idle":"2025-04-21T17:07:07.128823Z","shell.execute_reply.started":"2025-04-21T17:07:07.123043Z","shell.execute_reply":"2025-04-21T17:07:07.128132Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# **5- LSTM Decoder**","metadata":{}},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    \"\"\"LSTM decoder with attention mechanism\"\"\"\n    \n    def __init__(self, embed_size, hidden_size, vocab_size, attention_dim, encoder_dim=256, dropout=0.5):\n        super(DecoderRNN, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.attention = Attention(encoder_dim, hidden_size, attention_dim)\n        \n        # LSTM takes concatenated [embedded word + context vector]\n        self.lstm = nn.LSTM(embed_size + encoder_dim, hidden_size, batch_first=False)\n        \n        self.fc = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.encoder_dim = encoder_dim\n        self.hidden_size = hidden_size\n    \n    def forward(self, features, captions):\n        \"\"\"Generate caption predictions\"\"\"\n        batch_size = features.size(0)\n        \n        # Embed captions (seq_len, batch_size) -> (seq_len, batch_size, embed_size)\n        embeddings = self.dropout(self.embed(captions))\n        \n        # Initialize hidden state and cell state\n        h = torch.zeros(1, batch_size, self.hidden_size).to(device)\n        c = torch.zeros(1, batch_size, self.hidden_size).to(device)\n        \n        # Prepare output tensor (seq_len, batch_size, vocab_size)\n        seq_len = captions.size(0)\n        outputs = torch.zeros(seq_len, batch_size, self.fc.out_features).to(device)\n        \n        # Process each time step\n        for t in range(seq_len):\n            # Get context vector using attention\n            context, alpha = self.attention(features, h.squeeze(0))\n            \n            # Combine embedded word and context\n            lstm_input = torch.cat([embeddings[t], context], dim=1).unsqueeze(0)\n            \n            # LSTM step\n            lstm_out, (h, c) = self.lstm(lstm_input, (h, c))\n            \n            # Predict next word\n            output = self.fc(self.dropout(lstm_out.squeeze(0)))\n            outputs[t] = output\n        \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:07:23.652082Z","iopub.execute_input":"2025-04-21T17:07:23.652820Z","iopub.status.idle":"2025-04-21T17:07:23.659982Z","shell.execute_reply.started":"2025-04-21T17:07:23.652795Z","shell.execute_reply":"2025-04-21T17:07:23.659083Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# **6- Model Integration**","metadata":{}},{"cell_type":"code","source":"class CNNtoRNN(nn.Module):\n    \"\"\"Complete image captioning model combining CNN encoder and RNN decoder\"\"\"\n    \n    def __init__(self, embed_size, hidden_size, vocab_size, attention_dim):\n        super(CNNtoRNN, self).__init__()\n        self.encoder = EncoderCNN(embed_size)\n        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, attention_dim, embed_size)\n    \n    def forward(self, images, captions):\n        \"\"\"Forward pass through the entire model\"\"\"\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:07:37.190300Z","iopub.execute_input":"2025-04-21T17:07:37.191014Z","iopub.status.idle":"2025-04-21T17:07:37.195367Z","shell.execute_reply.started":"2025-04-21T17:07:37.190990Z","shell.execute_reply":"2025-04-21T17:07:37.194647Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# **7- Training Configuration**","metadata":{}},{"cell_type":"code","source":"def get_loader(root_folder, annotation_file, transform, batch_size=16, num_workers=4, shuffle=True):\n    \"\"\"Create data loader for the dataset\"\"\"\n    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n    \n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        collate_fn=MyCollate(pad_idx),\n        pin_memory=True\n    )\n    return loader, dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:07:46.909803Z","iopub.execute_input":"2025-04-21T17:07:46.910464Z","iopub.status.idle":"2025-04-21T17:07:46.914730Z","shell.execute_reply.started":"2025-04-21T17:07:46.910439Z","shell.execute_reply":"2025-04-21T17:07:46.913990Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Image transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to ResNet input size\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:07:59.003821Z","iopub.execute_input":"2025-04-21T17:07:59.004625Z","iopub.status.idle":"2025-04-21T17:07:59.008422Z","shell.execute_reply.started":"2025-04-21T17:07:59.004594Z","shell.execute_reply":"2025-04-21T17:07:59.007816Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Prepare data loader\nloader, dataset = get_loader(\n    root_folder=\"/kaggle/input/flickr8kimagescaptions/flickr8k/images\",\n    annotation_file=\"/kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\",\n    transform=transform\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:09.178133Z","iopub.execute_input":"2025-04-21T17:08:09.178742Z","iopub.status.idle":"2025-04-21T17:08:10.452115Z","shell.execute_reply.started":"2025-04-21T17:08:09.178718Z","shell.execute_reply":"2025-04-21T17:08:10.451589Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Model hyperparameters\nvocab_size = len(dataset.vocab)\nembed_size = 256\nhidden_size = 512\nattention_dim = 256\nlearning_rate = 3e-4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:08:19.088882Z","iopub.execute_input":"2025-04-21T17:08:19.089544Z","iopub.status.idle":"2025-04-21T17:08:19.093307Z","shell.execute_reply.started":"2025-04-21T17:08:19.089521Z","shell.execute_reply":"2025-04-21T17:08:19.092534Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Initialize model, loss, and optimizer\nmodel = CNNtoRNN(embed_size, hidden_size, vocab_size, attention_dim).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:52.162512Z","iopub.execute_input":"2025-04-21T17:09:52.162784Z","iopub.status.idle":"2025-04-21T17:09:52.715290Z","shell.execute_reply.started":"2025-04-21T17:09:52.162766Z","shell.execute_reply":"2025-04-21T17:09:52.714504Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\n\ndef train_model(model, loader, optimizer, criterion, num_epochs=10):\n    \"\"\"Training loop with mixed-precision training\"\"\"\n    model.train()\n    scaler = GradScaler()  # For mixed-precision training\n    \n    for epoch in range(num_epochs):\n        loop = tqdm(loader, total=len(loader), leave=True)\n        \n        for images, captions in loop:\n            images = images.to(device)\n            captions = captions.to(device)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass with mixed precision\n            with autocast():\n                # Predict next words (excluding EOS)\n                outputs = model(images, captions[:-1])\n                \n                # Calculate loss (excluding SOS)\n                loss = criterion(\n                    outputs.reshape(-1, outputs.shape[2]), \n                    captions[1:].reshape(-1)\n                )\n            \n            # Backward pass with gradient scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # Update progress bar\n            loop.set_postfix(loss=loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:09:55.714658Z","iopub.execute_input":"2025-04-21T17:09:55.714966Z","iopub.status.idle":"2025-04-21T17:09:55.721366Z","shell.execute_reply.started":"2025-04-21T17:09:55.714932Z","shell.execute_reply":"2025-04-21T17:09:55.720642Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Train the model\nnum_epochs = 10\ntrain_model(model, loader, optimizer, criterion, num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:10:45.531246Z","iopub.execute_input":"2025-04-21T17:10:45.531547Z","iopub.status.idle":"2025-04-21T17:51:05.791011Z","shell.execute_reply.started":"2025-04-21T17:10:45.531527Z","shell.execute_reply":"2025-04-21T17:51:05.790044Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2529/2529 [04:02<00:00, 10.45it/s, loss=3.44]\n100%|██████████| 2529/2529 [04:00<00:00, 10.53it/s, loss=4.21]\n100%|██████████| 2529/2529 [04:01<00:00, 10.49it/s, loss=2.77]\n100%|██████████| 2529/2529 [04:02<00:00, 10.43it/s, loss=3.06]\n100%|██████████| 2529/2529 [04:02<00:00, 10.42it/s, loss=2.4] \n100%|██████████| 2529/2529 [04:03<00:00, 10.41it/s, loss=2.92]\n100%|██████████| 2529/2529 [04:01<00:00, 10.45it/s, loss=3.06]\n100%|██████████| 2529/2529 [04:01<00:00, 10.47it/s, loss=2.59]\n100%|██████████| 2529/2529 [04:02<00:00, 10.42it/s, loss=2.24]\n100%|██████████| 2529/2529 [04:02<00:00, 10.43it/s, loss=2.62]\n","output_type":"stream"}],"execution_count":45}]}