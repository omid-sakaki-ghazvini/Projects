{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d32fb815141d4e229be2430ac6d20242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07bb94c664444dec80920545d183334e",
              "IPY_MODEL_f790f356dcd14ec6867d6c5beb5bc828",
              "IPY_MODEL_d8915ef97f4340b9bebb96a42c6d765d"
            ],
            "layout": "IPY_MODEL_bf6a8ffd04e644fabdc1780f0aa54009"
          }
        },
        "07bb94c664444dec80920545d183334e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_454658f438244c139e3401c74a73e79f",
            "placeholder": "​",
            "style": "IPY_MODEL_7ec3cc153f56432a9b703d0c39729202",
            "value": "Processing images: 100%"
          }
        },
        "f790f356dcd14ec6867d6c5beb5bc828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7951646a93a04a2297e6190977831a07",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_626ab5a5eb434653902450ca58c59cde",
            "value": 30
          }
        },
        "d8915ef97f4340b9bebb96a42c6d765d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aaa3f93bd274bf39a424476a74318a0",
            "placeholder": "​",
            "style": "IPY_MODEL_0f88264a3c434985b0b0ce68114ffb60",
            "value": " 30/30 [00:01&lt;00:00, 32.64it/s]"
          }
        },
        "bf6a8ffd04e644fabdc1780f0aa54009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "454658f438244c139e3401c74a73e79f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec3cc153f56432a9b703d0c39729202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7951646a93a04a2297e6190977831a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "626ab5a5eb434653902450ca58c59cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5aaa3f93bd274bf39a424476a74318a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f88264a3c434985b0b0ce68114ffb60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d054909745d2471395f9750f7c86c9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76fe0c34959543269d3b0f62877b0c8a",
              "IPY_MODEL_e2139771127849e4a17091840e5f1e43",
              "IPY_MODEL_1d1a7328dba640de98e57977603efc7f"
            ],
            "layout": "IPY_MODEL_f40661134ef44e4b8ca7e77c1df007a1"
          }
        },
        "76fe0c34959543269d3b0f62877b0c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73564fffc6674762a6cc41a92eacd72e",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a59bec738842749f28e7592586373a",
            "value": "Writing video frames: 100%"
          }
        },
        "e2139771127849e4a17091840e5f1e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c65a3f8edb8148ba983150c312d29967",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_997aa7d60a144d0dbc4c9bcb45e41c72",
            "value": 30
          }
        },
        "1d1a7328dba640de98e57977603efc7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a534081ba6734afc96237a55e7a94e64",
            "placeholder": "​",
            "style": "IPY_MODEL_59c095a6e6464cdfb2eee15fb6f56784",
            "value": " 30/30 [00:00&lt;00:00, 179.18it/s]"
          }
        },
        "f40661134ef44e4b8ca7e77c1df007a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73564fffc6674762a6cc41a92eacd72e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a59bec738842749f28e7592586373a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65a3f8edb8148ba983150c312d29967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "997aa7d60a144d0dbc4c9bcb45e41c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a534081ba6734afc96237a55e7a94e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59c095a6e6464cdfb2eee15fb6f56784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import glob\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import HTML, FileLink, display\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "!pip install -q ultralytics==8.3.36 kagglehub opencv-python-headless matplotlib tqdm seaborn\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import kagglehub\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('YOLOv12_BoneFracture')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4JQ9fTTC3zW",
        "outputId": "0e7e2b21-2be5-43ad-c609-6541e1c2b1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.3/887.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================="
      ],
      "metadata": {
        "id": "V62HK5aKDHHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_dir: str = None\n",
        "    output_dir: str = \"/content/output\"\n",
        "    epochs: int = 35\n",
        "    imgsz: int = 640\n",
        "    batch: int = 16\n",
        "    patience: int = 8\n",
        "    conf: float = 0.25\n",
        "    iou: float = 0.45\n",
        "    max_frames: int = 30\n",
        "    fps: int = 2\n",
        "\n",
        "    def __post_init__(self):\n",
        "        os.makedirs(self.output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "zunzktoQC6QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================================================\n",
        "# DATASET DOWNLOAD AND SETUP\n",
        "# =============================================================="
      ],
      "metadata": {
        "id": "_bk-Q1qGDKqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_dataset():\n",
        "    \"\"\"Download and setup the bone fracture dataset\"\"\"\n",
        "    DATASET_SLUG = \"jockeroika/human-bone-fractures-image-dataset\"\n",
        "    print(f\"Downloading dataset: {DATASET_SLUG}...\")\n",
        "\n",
        "    dataset_path = kagglehub.dataset_download(DATASET_SLUG)\n",
        "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "    # Auto-detect data directory\n",
        "    potential_roots = [\n",
        "        os.path.join(dataset_path, \"Human Bone Fractures Multi-modal Image Dataset (HBFMID)\", \"Bone Fractures Detection\"),\n",
        "        os.path.join(dataset_path, \"Bone Fractures Detection\"),\n",
        "        dataset_path\n",
        "    ]\n",
        "\n",
        "    DATA_DIR = None\n",
        "    for p in potential_roots:\n",
        "        if os.path.exists(p) and len(glob.glob(os.path.join(p, \"train\", \"images\", \"*.*\"))) > 0:\n",
        "            DATA_DIR = p\n",
        "            break\n",
        "\n",
        "    if DATA_DIR is None:\n",
        "        raise FileNotFoundError(f\"Could not locate train/images in {dataset_path}\")\n",
        "\n",
        "    print(f\"Data directory: {DATA_DIR}\")\n",
        "    return DATA_DIR\n",
        "\n",
        "DATA_DIR = setup_dataset()\n",
        "config = Config(data_dir=DATA_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYGNZKfRC80m",
        "outputId": "e178a04c-84ac-4676-bd34-0a5ae2fb45d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset: jockeroika/human-bone-fractures-image-dataset...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jockeroika/human-bone-fractures-image-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38.1M/38.1M [00:00<00:00, 162MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/jockeroika/human-bone-fractures-image-dataset/versions/1\n",
            "Data directory: /root/.cache/kagglehub/datasets/jockeroika/human-bone-fractures-image-dataset/versions/1/Human Bone Fractures Multi-modal Image Dataset (HBFMID)/Bone Fractures Detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================================================\n",
        "# CREATE DATA CONFIGURATION\n",
        "# =============================================================="
      ],
      "metadata": {
        "id": "wkosH4LqDN5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_yaml():\n",
        "    \"\"\"Create data.yaml configuration file\"\"\"\n",
        "    train_img_dir = glob.glob(os.path.join(config.data_dir, \"**\", \"train\", \"images\"), recursive=True)[0]\n",
        "    val_img_dir = glob.glob(os.path.join(config.data_dir, \"**\", \"valid\", \"images\"), recursive=True)[0]\n",
        "\n",
        "    print(f\"Train images: {train_img_dir}\")\n",
        "    print(f\"Val images: {val_img_dir}\")\n",
        "\n",
        "    # Define class names from the dataset\n",
        "    class_names = [\n",
        "        'elbow_positive', 'finger_positive', 'forearm_positive', 'humerus_positive',\n",
        "        'shoulder_positive', 'wrist_positive', 'elbow_negative', 'finger_negative',\n",
        "        'forearm_negative', 'humerus_negative', 'shoulder_negative', 'wrist_negative'\n",
        "    ]\n",
        "\n",
        "    # Build data configuration\n",
        "    root_for_yaml = os.path.dirname(config.data_dir)\n",
        "    data_yaml = {\n",
        "        'path': root_for_yaml,\n",
        "        'train': os.path.relpath(train_img_dir, root_for_yaml).replace(os.sep, '/'),\n",
        "        'val': os.path.relpath(val_img_dir, root_for_yaml).replace(os.sep, '/'),\n",
        "        'nc': len(class_names),\n",
        "        'names': class_names\n",
        "    }\n",
        "\n",
        "    yaml_path = os.path.join(config.output_dir, \"data.yaml\")\n",
        "    with open(yaml_path, \"w\") as f:\n",
        "        import yaml\n",
        "        yaml.dump(data_yaml, f, default_flow_style=False)\n",
        "\n",
        "    logger.info(f\"data.yaml created: {len(class_names)} classes\")\n",
        "    return yaml_path, class_names\n",
        "\n",
        "yaml_path, class_names = create_data_yaml()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz8D4zbRC_P9",
        "outputId": "3d183718-25ca-40ff-e6d7-1173c0988af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: /root/.cache/kagglehub/datasets/jockeroika/human-bone-fractures-image-dataset/versions/1/Human Bone Fractures Multi-modal Image Dataset (HBFMID)/Bone Fractures Detection/train/images\n",
            "Val images: /root/.cache/kagglehub/datasets/jockeroika/human-bone-fractures-image-dataset/versions/1/Human Bone Fractures Multi-modal Image Dataset (HBFMID)/Bone Fractures Detection/valid/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================================================\n",
        "# MODEL TRAINING\n",
        "# =============================================================="
      ],
      "metadata": {
        "id": "OFnVFEiQDQ82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    \"\"\"Train the YOLO model on bone fracture dataset\"\"\"\n",
        "    print(\"Starting model training...\")\n",
        "\n",
        "    # Use YOLOv8 as a reliable base model\n",
        "    model = YOLO(\"yolov8m.pt\")\n",
        "\n",
        "    results = model.train(\n",
        "        data=yaml_path,\n",
        "        epochs=config.epochs,\n",
        "        imgsz=config.imgsz,\n",
        "        batch=config.batch,\n",
        "        patience=config.patience,\n",
        "        lr0=0.01,\n",
        "        optimizer='AdamW',\n",
        "        weight_decay=0.0005,\n",
        "        momentum=0.937,\n",
        "        conf=config.conf,\n",
        "        iou=config.iou,\n",
        "        project=config.output_dir,\n",
        "        name=\"yolo_bone_fracture\",\n",
        "        exist_ok=True,\n",
        "        plots=True,\n",
        "        save=True,\n",
        "        device=0,\n",
        "        workers=2,\n",
        "        cache=False\n",
        "    )\n",
        "\n",
        "    # Get best model path\n",
        "    best_model_path = str(pathlib.Path(results.save_dir) / \"weights\" / \"best.pt\")\n",
        "    logger.info(f\"Training complete. Best model: {best_model_path}\")\n",
        "    return best_model_path\n",
        "\n",
        "best_model_path = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMF7YbXaDBuu",
        "outputId": "582710df-7302-4b3c-a89e-2af86ae7ba48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 323MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New https://pypi.org/project/ultralytics/8.3.221 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.36 🚀 Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/content/output/data.yaml, epochs=35, time=None, patience=8, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=2, project=/content/output, name=yolo_bone_fracture, exist_ok=True, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=0.25, iou=0.45, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/content/output/yolo_bone_fracture\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 27.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=12\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3782644  ultralytics.nn.modules.head.Detect           [12, [192, 384, 576]]         \n",
            "Model summary: 295 layers, 25,863,268 parameters, 25,863,252 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 469/475 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/output/yolo_bone_fracture', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 99.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/.cache/kagglehub/datasets/jockeroika/human-bone-fractures-image-dataset/versions/1/Human Bone Fractures Multi-modal Image Dataset (HBFMID)/Bone Fractures Detection/train/labels... 1347 images, 3 backgrounds, 0 corrupt: 100%|██████████| 1347/1347 [00:00<00:00, 1981.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /root/.cache/kagglehub/datasets/jockeroika/human-bone-fractures-image-dataset/versions/1/Human Bone Fractures Multi-modal Image Dataset (HBFMID)/Bone Fractures Detection/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /root/.cache/kagglehub/datasets/jockeroika/human-bone-fractures-image-dataset/versions/1/Human Bone Fractures Multi-modal Image Dataset (HBFMID)/Bone Fractures Detection/valid/labels... 128 images, 0 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<00:00, 925.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /root/.cache/kagglehub/datasets/jockeroika/human-bone-fractures-image-dataset/versions/1/Human Bone Fractures Multi-modal Image Dataset (HBFMID)/Bone Fractures Detection/valid/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/output/yolo_bone_fracture/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/output/yolo_bone_fracture\u001b[0m\n",
            "Starting training for 35 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/35      7.09G      2.883      5.139       2.73          3        640: 100%|██████████| 85/85 [00:46<00:00,  1.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157          0          0          0          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/35      7.01G      2.726      4.487       2.63         11        640: 100%|██████████| 85/85 [00:44<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157        0.1     0.0429   7.95e-05   1.36e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/35      7.13G      2.671       4.31      2.594          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.114    0.00795    0.00284   0.000777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/35      7.01G      2.637      4.139      2.601          8        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.114    0.00795    0.00284   0.000777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/35      7.12G      2.642      4.108      2.652          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.184    0.00154     0.0114    0.00341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/35      7.11G      2.563      3.953      2.585          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.172     0.0574     0.0419     0.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/35      7.12G      2.528      3.839      2.555          4        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.055     0.0221     0.0376      0.013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/35      7.01G      2.458      3.724       2.53          8        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0111    0.00769    0.00621    0.00237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/35      7.12G       2.43      3.626       2.49          6        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0556    0.00769     0.0311     0.0208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/35       7.1G      2.429      3.582      2.472          5        640: 100%|██████████| 85/85 [00:43<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.342     0.0385     0.0309    0.00972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/35      7.11G      2.401       3.48      2.453          4        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0235     0.0123     0.0152    0.00382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/35      7.01G      2.343      3.464      2.388          8        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0583     0.0108     0.0325     0.0114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/35      7.12G      2.362      3.397       2.42          8        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0881      0.011     0.0479     0.0252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/35      7.11G      2.323      3.291      2.397          5        640: 100%|██████████| 85/85 [00:43<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0425     0.0431     0.0346     0.0109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/35      7.11G      2.303      3.335      2.356          6        640: 100%|██████████| 85/85 [00:43<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157       0.05    0.00667     0.0267    0.00533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/35      7.01G      2.265      3.211      2.371          7        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0505      0.039     0.0423      0.017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/35      7.13G       2.24      3.259      2.323          4        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0505      0.039     0.0423      0.017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/35      7.11G      2.241      3.154      2.303          4        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.135     0.0331     0.0831     0.0263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/35       7.1G      2.209      3.125      2.284          5        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0273    0.00462     0.0144    0.00572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/35      7.01G      2.204      3.071      2.298          4        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157       0.13     0.0128     0.0694     0.0176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/35      7.14G      2.177      3.035      2.264          7        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.119     0.0272     0.0775     0.0282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/35      7.09G      2.166      2.995      2.251         11        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.163     0.0622      0.108     0.0399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/35      7.11G      2.135      2.916      2.233          6        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0125    0.00667    0.00667    0.00267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/35      7.01G      2.118      2.856      2.204          6        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0696     0.0246     0.0473     0.0194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/35      7.13G      2.075      2.867      2.196         10        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157     0.0833     0.0154     0.0483     0.0175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/35      7.11G      2.038       2.71      2.357          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.154     0.0467     0.0986     0.0318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/35      7.12G      2.007      2.611       2.35          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.153      0.112      0.148     0.0668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/35      7.02G      1.985      2.504      2.324          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.196     0.0787      0.139     0.0583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/35      7.13G      1.981      2.517      2.339          4        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.189     0.0977      0.144     0.0715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/35      7.12G      1.942      2.408      2.274          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.283      0.135      0.213     0.0909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/35      7.12G       1.92      2.309      2.268          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.285      0.147      0.214      0.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/35         7G      1.878      2.249       2.24          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157       0.36      0.142      0.257      0.126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/35      7.12G      1.881      2.196      2.222          5        640: 100%|██████████| 85/85 [00:43<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.322      0.144      0.239        0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/35       7.1G      1.849      2.176      2.229          3        640: 100%|██████████| 85/85 [00:43<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.293        0.2      0.263      0.117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/35       7.1G      1.823      2.098      2.201          4        640: 100%|██████████| 85/85 [00:43<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.312      0.231      0.276      0.127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "35 epochs completed in 0.476 hours.\n",
            "Optimizer stripped from /content/output/yolo_bone_fracture/weights/last.pt, 52.0MB\n",
            "Optimizer stripped from /content/output/yolo_bone_fracture/weights/best.pt, 52.0MB\n",
            "\n",
            "Validating /content/output/yolo_bone_fracture/weights/best.pt...\n",
            "Ultralytics 8.3.36 🚀 Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 218 layers, 25,846,708 parameters, 0 gradients, 78.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        128        157      0.312      0.231      0.276      0.128\n",
            "        elbow_positive         15         15      0.571      0.533      0.667      0.423\n",
            "       finger_positive         10         10          0          0          0          0\n",
            "      forearm_positive          7          7      0.364      0.571      0.435      0.174\n",
            "      humerus_positive          1          1          0          0          0          0\n",
            "     shoulder_positive         30         30       0.45        0.3      0.334      0.149\n",
            "        wrist_positive          7          7          1      0.429      0.714       0.25\n",
            "        elbow_negative          3          3          0          0          0          0\n",
            "       finger_negative          5          5          0          0          0          0\n",
            "      forearm_negative         53         65      0.738      0.477      0.613      0.285\n",
            "      humerus_negative         11         14          0          0          0          0\n",
            "Speed: 0.5ms preprocess, 12.1ms inference, 0.0ms loss, 5.2ms postprocess per image\n",
            "Results saved to \u001b[1m/content/output/yolo_bone_fracture\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================================================\n",
        "# INFERENCE AND VIDEO GENERATION\n",
        "# =============================================================="
      ],
      "metadata": {
        "id": "46xOkR_cDUf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_detection_video():\n",
        "    \"\"\"Create detection video with automatic download\"\"\"\n",
        "\n",
        "    print(\"🎥 Starting video generation...\")\n",
        "\n",
        "    # Load the trained model\n",
        "    model = YOLO(best_model_path)\n",
        "    val_img_dir = glob.glob(os.path.join(config.data_dir, \"**\", \"valid\", \"images\"), recursive=True)[0]\n",
        "    val_images = sorted(glob.glob(os.path.join(val_img_dir, \"*.*\")))[:config.max_frames]\n",
        "\n",
        "    print(f\"Processing {len(val_images)} images for video...\")\n",
        "\n",
        "    def draw_detections(img_path, model, img_size=(640, 640)):\n",
        "        \"\"\"Draw detections on a single image\"\"\"\n",
        "        try:\n",
        "            # Read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                img = np.random.randint(100, 200, (*img_size, 3), dtype=np.uint8)\n",
        "            else:\n",
        "                img = cv2.resize(img, img_size)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        except:\n",
        "            img = np.random.randint(100, 200, (*img_size, 3), dtype=np.uint8)\n",
        "\n",
        "        # Run inference\n",
        "        results = model.predict(\n",
        "            source=img_path,\n",
        "            conf=0.15,  # Lower confidence for better detection\n",
        "            iou=0.4,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Draw detections\n",
        "        for r in results:\n",
        "            if r.boxes is not None and len(r.boxes) > 0:\n",
        "                boxes = r.boxes.data.cpu().numpy()\n",
        "                for box in boxes:\n",
        "                    x1, y1, x2, y2, conf, cls_id = box\n",
        "                    x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "\n",
        "                    if x2 > x1 and y2 > y1:  # Validate coordinates\n",
        "                        cls_id = int(cls_id)\n",
        "                        label = f\"{class_names[cls_id] if cls_id < len(class_names) else 'unknown'} {conf:.2f}\"\n",
        "\n",
        "                        # Generate color based on class\n",
        "                        color = plt.cm.Set3(cls_id % 12)[:3]\n",
        "                        color = tuple(int(c*255) for c in color)\n",
        "\n",
        "                        # Draw bounding box\n",
        "                        cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
        "\n",
        "                        # Draw label\n",
        "                        (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "                        cv2.rectangle(img, (x1, y1 - th - 10), (x1 + tw, y1), color, -1)\n",
        "                        cv2.putText(img, label, (x1, y1-5),\n",
        "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "            else:\n",
        "                # Add \"No Detection\" text if no detections found\n",
        "                cv2.putText(img, \"No Fracture Detected\", (50, 50),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        return img\n",
        "\n",
        "    # Generate frames\n",
        "    frames = []\n",
        "    print(\"Generating frames...\")\n",
        "\n",
        "    for i, img_path in enumerate(tqdm(val_images, desc=\"Processing images\")):\n",
        "        try:\n",
        "            frame = draw_detections(img_path, model)\n",
        "            frames.append(frame)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {i}: {e}\")\n",
        "            # Create placeholder frame\n",
        "            placeholder = np.random.randint(100, 200, (640, 640, 3), dtype=np.uint8)\n",
        "            cv2.putText(placeholder, \"Processing Error\", (50, 50),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "            frames.append(placeholder)\n",
        "\n",
        "    # Create video\n",
        "    if frames:\n",
        "        video_path = \"/content/bone_fracture_detection.mp4\"\n",
        "        h, w = frames[0].shape[:2]\n",
        "\n",
        "        # Use better video codec\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        writer = cv2.VideoWriter(video_path, fourcc, config.fps, (w, h))\n",
        "\n",
        "        print(\"Creating video...\")\n",
        "        for frame in tqdm(frames, desc=\"Writing video frames\"):\n",
        "            # Convert RGB to BGR for OpenCV\n",
        "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "            writer.write(frame_bgr)\n",
        "\n",
        "        writer.release()\n",
        "        print(f\"✅ Video created: {video_path}\")\n",
        "\n",
        "        # Auto-download the video\n",
        "        if os.path.exists(video_path):\n",
        "            file_size = os.path.getsize(video_path) / (1024 * 1024)  # Size in MB\n",
        "            print(f\"📦 Video file size: {file_size:.1f} MB\")\n",
        "\n",
        "            # Create download link\n",
        "            print(\"🔽 Downloading video automatically...\")\n",
        "            display(FileLink(video_path))\n",
        "\n",
        "            # Alternative download method\n",
        "            from google.colab import files\n",
        "            files.download(video_path)\n",
        "\n",
        "            return video_path\n",
        "        else:\n",
        "            print(\"❌ Video file was not created\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"❌ No frames were generated\")\n",
        "        return None\n",
        "\n",
        "# Generate and download the video\n",
        "video_path = create_detection_video()\n",
        "\n",
        "if video_path:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎉 VIDEO GENERATION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"📹 Output: bone_fracture_detection.mp4\")\n",
        "    print(f\"🖼️  Frames: {config.max_frames}\")\n",
        "    print(f\"🎬 FPS: {config.fps}\")\n",
        "    print(f\"💾 Location: {video_path}\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"\\n❌ Video generation failed. Please check the error messages above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383,
          "referenced_widgets": [
            "d32fb815141d4e229be2430ac6d20242",
            "07bb94c664444dec80920545d183334e",
            "f790f356dcd14ec6867d6c5beb5bc828",
            "d8915ef97f4340b9bebb96a42c6d765d",
            "bf6a8ffd04e644fabdc1780f0aa54009",
            "454658f438244c139e3401c74a73e79f",
            "7ec3cc153f56432a9b703d0c39729202",
            "7951646a93a04a2297e6190977831a07",
            "626ab5a5eb434653902450ca58c59cde",
            "5aaa3f93bd274bf39a424476a74318a0",
            "0f88264a3c434985b0b0ce68114ffb60",
            "d054909745d2471395f9750f7c86c9a5",
            "76fe0c34959543269d3b0f62877b0c8a",
            "e2139771127849e4a17091840e5f1e43",
            "1d1a7328dba640de98e57977603efc7f",
            "f40661134ef44e4b8ca7e77c1df007a1",
            "73564fffc6674762a6cc41a92eacd72e",
            "f1a59bec738842749f28e7592586373a",
            "c65a3f8edb8148ba983150c312d29967",
            "997aa7d60a144d0dbc4c9bcb45e41c72",
            "a534081ba6734afc96237a55e7a94e64",
            "59c095a6e6464cdfb2eee15fb6f56784"
          ]
        },
        "id": "IrKuLzW4CvuO",
        "outputId": "a5bd8e89-94aa-47af-f20b-a98b575e30f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎥 Starting video generation...\n",
            "Processing 30 images for video...\n",
            "Generating frames...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing images:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d32fb815141d4e229be2430ac6d20242"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating video...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Writing video frames:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d054909745d2471395f9750f7c86c9a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Video created: /content/bone_fracture_detection.mp4\n",
            "📦 Video file size: 0.8 MB\n",
            "🔽 Downloading video automatically...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/content/bone_fracture_detection.mp4"
            ],
            "text/html": [
              "<a href='/content/bone_fracture_detection.mp4' target='_blank'>/content/bone_fracture_detection.mp4</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0bf90dee-7ce8-4f0d-b31e-bf2a86599c8a\", \"bone_fracture_detection.mp4\", 867110)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🎉 VIDEO GENERATION COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "📹 Output: bone_fracture_detection.mp4\n",
            "🖼️  Frames: 30\n",
            "🎬 FPS: 2\n",
            "💾 Location: /content/bone_fracture_detection.mp4\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}