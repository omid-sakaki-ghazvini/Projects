{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install required packages**"
      ],
      "metadata": {
        "id": "m5nw_O_MWusv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow tensorflow-hub opencv-python mediapipe scipy\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import mediapipe as mp\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from google.colab import files\n",
        "import os"
      ],
      "metadata": {
        "id": "J1LUxhd9W2Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MoveNet model**"
      ],
      "metadata": {
        "id": "RFbEGqQ6W6Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\").signatures['serving_default']"
      ],
      "metadata": {
        "id": "rponXfVQW97_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MediaPipe model**"
      ],
      "metadata": {
        "id": "MfdSYHrEXAin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "pose_mp = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)"
      ],
      "metadata": {
        "id": "Xcjy3FC-XEX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Keypoint Dictionary and Edge List**"
      ],
      "metadata": {
        "id": "wjiIzwXsXHkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KEYPOINT_DICT = {\n",
        "    'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4,\n",
        "    'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8,\n",
        "    'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12,\n",
        "    'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16\n",
        "}\n",
        "EDGE_LIST = [\n",
        "    (0, 1), (0, 2), (1, 3), (2, 4),   # Head\n",
        "    (5, 6),                           # Shoulders\n",
        "    (5, 7), (7, 9),                   # Left arm\n",
        "    (6, 8), (8, 10),                  # Right arm\n",
        "    (11, 12),                         # Hips\n",
        "    (11, 13), (13, 15),               # Left leg\n",
        "    (12, 14), (14, 16)                # Right leg\n",
        "]"
      ],
      "metadata": {
        "id": "3AX8XGfYXOwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MoveNet**"
      ],
      "metadata": {
        "id": "29GmzIBGXoOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_movenet_orientation(keypoints, frame_shape):\n",
        "    h, w = frame_shape\n",
        "    corrected = keypoints.copy()\n",
        "    shoulder_vec = keypoints[6] - keypoints[5]\n",
        "    angle = np.degrees(np.arctan2(shoulder_vec[1], shoulder_vec[0]))\n",
        "    if abs(angle) < 45:\n",
        "        rotation_angle = -angle\n",
        "        rot = R.from_euler('z', rotation_angle, degrees=True)\n",
        "        torso_center = (keypoints[5] + keypoints[6] + keypoints[11] + keypoints[12]) / 4\n",
        "        centered = keypoints - torso_center\n",
        "        corrected = rot.apply(centered) + torso_center\n",
        "        corrected[:, 0] = np.clip(corrected[:, 0], 0, w-1)\n",
        "        corrected[:, 1] = np.clip(corrected[:, 1], 0, h-1)\n",
        "    if corrected[5][1] > corrected[11][1]:\n",
        "        corrected[:, 1] = h - corrected[:, 1]\n",
        "    return corrected\n",
        "\n",
        "def get_movenet_keypoints(frame):\n",
        "    h, w = frame.shape[:2]\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    input_image = tf.image.resize_with_pad(tf.expand_dims(frame_rgb, axis=0), 256, 256)\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    outputs = movenet(input_image)\n",
        "    kps = outputs['output_0'].numpy()[0, 0, :, :]\n",
        "    keypoints = np.zeros((17,2), dtype=np.float32)\n",
        "    keypoints[:, 0] = kps[:,1] * w  # x\n",
        "    keypoints[:, 1] = kps[:,0] * h  # y\n",
        "    scores = kps[:,2]\n",
        "    keypoints = correct_movenet_orientation(keypoints, (h, w))\n",
        "    return keypoints, scores"
      ],
      "metadata": {
        "id": "2MZuCKrNXUDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MediaPipe**"
      ],
      "metadata": {
        "id": "jC_DmKN5Xirn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mediapipe_keypoints(frame):\n",
        "    h, w = frame.shape[:2]\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose_mp.process(frame_rgb)\n",
        "    keypoints = np.zeros((17, 2), dtype=np.float32)\n",
        "    scores = np.zeros(17, dtype=np.float32)\n",
        "    if results.pose_landmarks:\n",
        "        mapping = [\n",
        "            mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.LEFT_EYE, mp_pose.PoseLandmark.RIGHT_EYE,\n",
        "            mp_pose.PoseLandmark.LEFT_EAR, mp_pose.PoseLandmark.RIGHT_EAR,\n",
        "            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
        "            mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.RIGHT_ELBOW,\n",
        "            mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,\n",
        "            mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n",
        "            mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.RIGHT_KNEE,\n",
        "            mp_pose.PoseLandmark.LEFT_ANKLE, mp_pose.PoseLandmark.RIGHT_ANKLE,\n",
        "        ]\n",
        "        for i, idx in enumerate(mapping):\n",
        "            lm = results.pose_landmarks.landmark[idx]\n",
        "            keypoints[i] = [lm.x * w, lm.y * h]\n",
        "            scores[i] = lm.visibility if hasattr(lm, \"visibility\") else 1.0\n",
        "    return keypoints, scores"
      ],
      "metadata": {
        "id": "xXkwhpXrXWof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_skeleton(frame, keypoints, scores, label, point_color, edge_color):\n",
        "    h, w = frame.shape[:2]\n",
        "    # Draw edges\n",
        "    for (i, j) in EDGE_LIST:\n",
        "        if scores[i] > 0.3 and scores[j] > 0.3:\n",
        "            pt1 = tuple(np.round(keypoints[i]).astype(int))\n",
        "            pt2 = tuple(np.round(keypoints[j]).astype(int))\n",
        "            cv2.line(frame, pt1, pt2, edge_color, 2)\n",
        "    # Draw points\n",
        "    for i, (x, y) in enumerate(keypoints):\n",
        "        if scores[i] > 0.3:\n",
        "            cv2.circle(frame, (int(x), int(y)), 5, point_color, -1)\n",
        "    # Label\n",
        "    cv2.rectangle(frame, (0,0), (w,40), (40,40,40), -1)\n",
        "    cv2.putText(frame, label, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2)\n",
        "    return frame"
      ],
      "metadata": {
        "id": "0P22lrJBXZJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_dual(input_path, output_path_mp, output_path_mn):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    out_mp = cv2.VideoWriter(output_path_mp, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "    out_mn = cv2.VideoWriter(output_path_mn, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Processing {frame_count} frames...\")\n",
        "    idx = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # MediaPipe: Red points, white edges\n",
        "        mp_kps, mp_scores = get_mediapipe_keypoints(frame)\n",
        "        mp_frame = frame.copy()\n",
        "        mp_frame = draw_skeleton(mp_frame, mp_kps, mp_scores, \"MediaPipe Pose\", (0,0,255), (255,255,255))\n",
        "        out_mp.write(mp_frame)\n",
        "        # MoveNet: Blue points, white edges\n",
        "        mn_kps, mn_scores = get_movenet_keypoints(frame)\n",
        "        mn_frame = frame.copy()\n",
        "        mn_frame = draw_skeleton(mn_frame, mn_kps, mn_scores, \"MoveNet Pose\", (255,0,0), (255,255,255))\n",
        "        out_mn.write(mn_frame)\n",
        "        idx += 1\n",
        "        if idx % 30 == 0: print(f\"Processed {idx}/{frame_count} frames...\")\n",
        "    cap.release()\n",
        "    out_mp.release()\n",
        "    out_mn.release()\n",
        "    print(f\"MediaPipe output saved to: {output_path_mp}\")\n",
        "    print(f\"MoveNet output saved to: {output_path_mn}\")"
      ],
      "metadata": {
        "id": "EYJBxf2lXdBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4rJ3IKbMaGk"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"Pose Estimation Videos: MediaPipe (red points) and MoveNet Thunder (blue points), both with white edges\")\n",
        "    print(\"Please upload your video file (preferably MP4):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No video uploaded\")\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "    output_path_mp = \"pose_mediapipe.mp4\"\n",
        "    output_path_mn = \"pose_movenet.mp4\"\n",
        "    print(\"Processing video. Please wait...\")\n",
        "    process_video_dual(video_path, output_path_mp, output_path_mn)\n",
        "    print(\"Download MediaPipe result:\")\n",
        "    files.download(output_path_mp)\n",
        "    print(\"Download MoveNet result:\")\n",
        "    files.download(output_path_mn)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}