{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Qwen2.5-3B Persian AI Assistant**"
      ],
      "metadata": {
        "id": "HSxVMwzm_WT1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJW_duRN3fpT"
      },
      "outputs": [],
      "source": [
        "# ===========================================================\n",
        "# SECTION 1: INSTALLATION\n",
        "# ===========================================================\n",
        "print(\"üöÄ Setting up Google Colab environment...\")\n",
        "\n",
        "!pip install -q --upgrade pip setuptools wheel\n",
        "!pip install -q transformers==4.44.2\n",
        "!pip install -q \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q --no-deps trl peft accelerate bitsandbytes datasets xformers\n",
        "\n",
        "import os, gc, torch\n",
        "from datetime import datetime\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ Colab GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 2: CONFIGURATION\n",
        "# ===========================================================\n",
        "HF_TOKEN = \"Replace with your token\"  # Replace with your token\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "DOMAINS = [\"Artificial Intelligence\", \"Technology and Innovation\"]\n",
        "\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "OUTPUT_DIR = \"/content/qwen2.5-3b-persian-ai-tech-final\"\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 3: LOGIN & MODEL LOADING\n",
        "# ===========================================================\n",
        "print(\"\\nüîê Logging into HuggingFace...\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "\n",
        "print(\"\\nü§ñ Loading 4-bit quantized model...\")\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 4: LoRA CONFIGURATION\n",
        "# ===========================================================\n",
        "print(\"\\nüéØ Applying LoRA...\")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 5: LOAD DATASET\n",
        "# ===========================================================\n",
        "print(\"\\nüìä Loading ParsBench dataset...\")\n",
        "\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "datasets = []\n",
        "for d in DOMAINS:\n",
        "    print(f\"  Loading: {d}\")\n",
        "    try:\n",
        "        ds = load_dataset(\"ParsBench/PersianSyntheticQA\", name=d, split=\"train[:2000]\")\n",
        "        datasets.append(ds)\n",
        "        print(f\"    Loaded {len(ds)} samples\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Error: {e}\")\n",
        "        continue\n",
        "\n",
        "if not datasets:\n",
        "    raise ValueError(\"‚ùå Could not load any dataset!\")\n",
        "\n",
        "dataset = concatenate_datasets(datasets)\n",
        "print(f\"‚úÖ Total samples: {len(dataset)}\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 6: FORMAT DATA\n",
        "# ===========================================================\n",
        "def format_chat(examples):\n",
        "    texts = []\n",
        "    for msgs in examples[\"messages\"]:\n",
        "        text = \"<|im_start|>system\\nÿ¥ŸÖÿß €å⁄© ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ ŸÅÿßÿ±ÿ≥€å Ÿáÿ≥ÿ™€åÿØ.<|im_end|>\\n\"\n",
        "        for m in msgs:\n",
        "            if m[\"role\"] == \"system\":\n",
        "                continue\n",
        "            role = \"user\" if m[\"role\"] == \"user\" else \"assistant\"\n",
        "            text += f\"<|im_start|>{role}\\n{m['content']}<|im_end|>\\n\"\n",
        "        text += tokenizer.eos_token\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"Formatting dataset...\")\n",
        "dataset = dataset.map(format_chat, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "# Split\n",
        "if len(dataset) > 100:\n",
        "    split_dataset = dataset.train_test_split(test_size=0.1, seed=3407)\n",
        "    train_dataset = split_dataset[\"train\"]\n",
        "    eval_dataset = split_dataset[\"test\"]\n",
        "    print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
        "else:\n",
        "    train_dataset = dataset\n",
        "    eval_dataset = dataset[:50]\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 7: TRAINING SETUP (FIXED PARAMETER NAMES)\n",
        "# ===========================================================\n",
        "print(\"\\n‚öôÔ∏è Setting up training...\")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# CORRECTED TrainingArguments with proper parameter names\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    warmup_steps=10,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=3407,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True,\n",
        "    remove_unused_columns=True,\n",
        "    dataloader_pin_memory=True,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 8: TRAIN THE MODEL\n",
        "# ===========================================================\n",
        "print(\"\\nüéì Starting training...\")\n",
        "print(\"   This will take 1-2 hours on Colab T4\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 9: SAVE MODEL\n",
        "# ===========================================================\n",
        "print(\"\\nüíæ Saving model...\")\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 10: TEST\n",
        "# ===========================================================\n",
        "print(\"\\nüß™ Testing model...\")\n",
        "\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "def ask(question):\n",
        "    prompt = \"<|im_start|>system\\nÿ¥ŸÖÿß €å⁄© ÿØÿ≥ÿ™€åÿßÿ± ŸáŸàÿ¥ŸÖŸÜÿØ ŸÅÿßÿ±ÿ≥€å Ÿáÿ≥ÿ™€åÿØ.<|im_end|>\\n\"\n",
        "    prompt += f\"<|im_start|>user\\n{question}<|im_end|>\\n\"\n",
        "    prompt += \"<|im_start|>assistant\\n\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nTesting Persian responses:\")\n",
        "test_questions = [\"ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ⁄Ü€åÿ≥ÿ™ÿü\", \"ÿ™ŸÅÿßŸàÿ™ AI Ÿà ML ⁄Ü€åÿ≥ÿ™ÿü\"]\n",
        "for q in test_questions:\n",
        "    try:\n",
        "        answer = ask(q)\n",
        "        print(f\"\\n‚ùì {q}\")\n",
        "        print(f\"üí¨ {answer[:150]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùì {q}\")\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ Persian AI Assistant training completed!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Upload trained model to Hugging Face Hub**"
      ],
      "metadata": {
        "id": "nlgK1EG5_1vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÄ Uploading model to Hugging Face Hub...\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 1: SETUP & CONFIGURATION\n",
        "# ===========================================================\n",
        "import os\n",
        "from huggingface_hub import HfApi, create_repo, login\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "HF_TOKEN = \"Replace with your token\"  # Replace with your token\n",
        "MODEL_PATH = \"/content/qwen2.5-3b-persian-ai-tech-final\"\n",
        "\n",
        "# Repository name - CHANGE THIS!\n",
        "REPO_NAME = \"OmidSakaki/qwen2.5-3b-persian-ai-tech\"  # Change \"your-username\"\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 2: LOGIN TO HUGGING FACE\n",
        "# ===========================================================\n",
        "print(\"üîê Logging into Hugging Face...\")\n",
        "try:\n",
        "    login(token=HF_TOKEN, add_to_git_credential=True)\n",
        "    print(\"‚úÖ Login successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Login failed: {e}\")\n",
        "    print(\"Please check your HF_TOKEN\")\n",
        "    exit()\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 3: CREATE REPOSITORY\n",
        "# ===========================================================\n",
        "print(f\"\\nüì¶ Creating repository: {REPO_NAME}\")\n",
        "try:\n",
        "    create_repo(\n",
        "        repo_id=REPO_NAME,\n",
        "        token=HF_TOKEN,\n",
        "        private=True,\n",
        "        repo_type=\"model\",\n",
        "        exist_ok=True,\n",
        "    )\n",
        "    print(\"‚úÖ Repository created!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Repository creation: {e}\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 4: UPLOAD MODEL FILES\n",
        "# ===========================================================\n",
        "print(\"\\nüì§ Uploading model files...\")\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚ùå Model directory not found: {MODEL_PATH}\")\n",
        "    exit()\n",
        "\n",
        "try:\n",
        "    api.upload_folder(\n",
        "        folder_path=MODEL_PATH,\n",
        "        repo_id=REPO_NAME,\n",
        "        repo_type=\"model\",\n",
        "        commit_message=\"Qwen2.5-3B Persian AI Assistant - Trained on Colab\",\n",
        "    )\n",
        "    print(\"‚úÖ Model uploaded!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Upload failed: {e}\")\n",
        "    print(\"\\nTrying file-by-file upload...\")\n",
        "\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(MODEL_PATH):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                rel_path = os.path.relpath(file_path, MODEL_PATH)\n",
        "\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    api.upload_file(\n",
        "                        path_or_fileobj=f,\n",
        "                        path_in_repo=rel_path,\n",
        "                        repo_id=REPO_NAME,\n",
        "                        repo_type=\"model\",\n",
        "                    )\n",
        "                print(f\"  Uploaded: {rel_path}\")\n",
        "\n",
        "        print(\"‚úÖ All files uploaded!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå File upload failed: {e2}\")\n"
      ],
      "metadata": {
        "id": "NFF7GQn3LGv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Persian RAG System with Qwen2.5-3B Model**"
      ],
      "metadata": {
        "id": "PN1DGCA_AOPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# SECTION 1: INSTALLATION & SETUP\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "This section installs only the essential packages with compatible versions\n",
        "to avoid dependency conflicts. We use CPU-only PyTorch and specific versions\n",
        "that work well together on Google Colab.\n",
        "\"\"\"\n",
        "\n",
        "print(\"üì¶ Installing essential packages...\")\n",
        "\n",
        "# Install minimal PyTorch (CPU version) - compatible with Colab\n",
        "!pip install -q torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# Install transformers with specific version to avoid compatibility issues\n",
        "!pip install -q transformers==4.35.0\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully!\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 2: IMPORT LIBRARIES\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Import necessary libraries after installation.\n",
        "Keep imports minimal to reduce potential import errors.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(f\"\\nüîß Environment check:\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")  # Should be False for CPU-only\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 3: MODEL CONFIGURATION\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Configuration for the Persian Qwen2.5-3B model.\n",
        "This model is specifically trained for Persian language tasks.\n",
        "\"\"\"\n",
        "\n",
        "MODEL_NAME = \"OmidSakaki/qwen2.5-3b-persian-ai-tech\"\n",
        "\n",
        "print(f\"\\nü§ñ Model to load: {MODEL_NAME}\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 4: PERSIAN KNOWLEDGE BASE\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Simple Persian knowledge base for the RAG system.\n",
        "Contains factual information about AI/ML topics in Persian.\n",
        "\"\"\"\n",
        "\n",
        "PERSIAN_KNOWLEDGE = [\n",
        "    \"ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å (AI) ÿ¥ÿßÿÆŸá‚Äåÿß€å ÿßÿ≤ ÿπŸÑŸàŸÖ ⁄©ÿßŸÖŸæ€åŸàÿ™ÿ± ÿßÿ≥ÿ™ ⁄©Ÿá ÿ®Ÿá ÿ≥ÿßÿÆÿ™ ŸÖÿßÿ¥€åŸÜ‚ÄåŸáÿß€å ŸáŸàÿ¥ŸÖŸÜÿØ ŸÖ€å‚ÄåŸæÿ±ÿØÿßÿ≤ÿØ.\",\n",
        "    \"€åÿßÿØ⁄Ø€åÿ±€å ŸÖÿßÿ¥€åŸÜ ÿ≥Ÿá ŸÜŸàÿπ ÿßÿµŸÑ€å ÿØÿßÿ±ÿØ: €±. ŸÜÿ∏ÿßÿ±ÿ™ ÿ¥ÿØŸá €≤. ÿ®ÿØŸàŸÜ ŸÜÿ∏ÿßÿ±ÿ™ €≥. ÿ™ŸÇŸà€åÿ™€å\",\n",
        "    \"Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ≤ÿ®ÿßŸÜ ÿ∑ÿ®€åÿπ€å (NLP) ÿ®ÿ±ÿß€å ÿ™ÿ±ÿ¨ŸÖŸá ŸÖÿßÿ¥€åŸÜ€å Ÿà ⁄Üÿ™‚Äåÿ®ÿßÿ™‚ÄåŸáÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ.\",\n",
        "    \"RAG (Retrieval-Augmented Generation) ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ±ÿß ÿ®ÿßÿ≤€åÿßÿ®€å ⁄©ÿ±ÿØŸá Ÿà ÿ≥Ÿæÿ≥ Ÿæÿßÿ≥ÿÆ ÿ™ŸàŸÑ€åÿØ ŸÖ€å‚Äå⁄©ŸÜÿØ.\",\n",
        "    \"ŸÖÿπŸÖÿßÿ±€å ÿ™ÿ±ŸÜÿ≥ŸÅŸàÿ±ŸÖÿ± ÿ®ÿ± Ÿæÿß€åŸá ŸÖ⁄©ÿßŸÜ€åÿ≥ŸÖ ÿ™Ÿàÿ¨Ÿá (Attention Mechanism) ⁄©ÿßÿ± ŸÖ€å‚Äå⁄©ŸÜÿØ.\"\n",
        "]\n",
        "\n",
        "print(f\"üìö Knowledge base created with {len(PERSIAN_KNOWLEDGE)} Persian documents\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 5: SIMPLE TEXT RETRIEVAL FUNCTION\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Basic text retrieval without vector databases.\n",
        "Uses simple keyword matching to find relevant documents.\n",
        "\"\"\"\n",
        "\n",
        "def retrieve_relevant_documents(question, documents, top_k=2):\n",
        "    \"\"\"\n",
        "    Find relevant documents based on word overlap with the question.\n",
        "\n",
        "    Args:\n",
        "        question (str): User's question in Persian\n",
        "        documents (list): List of Persian knowledge documents\n",
        "        top_k (int): Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        list: Relevant documents sorted by relevance\n",
        "    \"\"\"\n",
        "    # Convert to lowercase and split into words\n",
        "    question_words = set(question.lower().split())\n",
        "\n",
        "    # Score each document based on word overlap\n",
        "    scored_documents = []\n",
        "\n",
        "    for doc in documents:\n",
        "        doc_words = set(doc.lower().split())\n",
        "        common_words = len(question_words.intersection(doc_words))\n",
        "        scored_documents.append((common_words, doc))\n",
        "\n",
        "    # Sort by relevance (highest score first)\n",
        "    scored_documents.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # Return only the content (without scores)\n",
        "    relevant_docs = []\n",
        "    for score, doc in scored_documents[:top_k]:\n",
        "        if score > 0:  # Only include documents with some relevance\n",
        "            relevant_docs.append(doc)\n",
        "\n",
        "    return relevant_docs\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 6: LOAD QWEN2.5 PERSIAN MODEL\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Load the Qwen2.5-3B Persian model with error handling.\n",
        "If model loading fails, the system will use a fallback mode.\n",
        "\"\"\"\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    Attempt to load the Qwen2.5 Persian model with fallback options.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (tokenizer, model, model_loaded_flag)\n",
        "    \"\"\"\n",
        "    print(\"\\nü§ñ Loading Qwen2.5 Persian model...\")\n",
        "\n",
        "    try:\n",
        "        # First attempt: Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        # Then load model with CPU settings\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            torch_dtype=torch.float32,      # Use float32 for CPU\n",
        "            device_map=\"cpu\",               # Force CPU usage\n",
        "            low_cpu_mem_usage=True          # Optimize for CPU memory\n",
        "        )\n",
        "\n",
        "        # Set padding token if not present\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        print(\"‚úÖ Model loaded successfully on CPU!\")\n",
        "        return tokenizer, model, True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Model loading failed: {str(e)[:100]}...\")\n",
        "        print(\"Using fallback mode for demonstration...\")\n",
        "\n",
        "        # Create mock tokenizer and model for fallback\n",
        "        class MockTokenizer:\n",
        "            def __init__(self):\n",
        "                self.pad_token_id = 0\n",
        "\n",
        "            def __call__(self, text, return_tensors=\"pt\", **kwargs):\n",
        "                return {\n",
        "                    \"input_ids\": torch.tensor([[101, 102, 103, 104, 105]]),\n",
        "                    \"attention_mask\": torch.tensor([[1, 1, 1, 1, 1]])\n",
        "                }\n",
        "\n",
        "            def decode(self, tokens, skip_special_tokens=True):\n",
        "                return \"This is a mock response. Model could not be loaded.\"\n",
        "\n",
        "        class MockModel:\n",
        "            def generate(self, **kwargs):\n",
        "                return torch.tensor([[1, 2, 3, 4, 5]])\n",
        "\n",
        "        return MockTokenizer(), MockModel(), False\n",
        "\n",
        "# Load the model\n",
        "tokenizer, model, model_loaded = load_model()\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 7: RAG ANSWER GENERATION\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Generate answers using retrieved context and the language model.\n",
        "Combines information retrieval with text generation.\n",
        "\"\"\"\n",
        "\n",
        "def generate_rag_answer(question, use_model=True):\n",
        "    \"\"\"\n",
        "    Generate answer using RAG approach.\n",
        "\n",
        "    Args:\n",
        "        question (str): User's question in Persian\n",
        "        use_model (bool): Whether to use the real model or fallback\n",
        "\n",
        "    Returns:\n",
        "        str: Generated answer in Persian\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve relevant documents\n",
        "    relevant_docs = retrieve_relevant_documents(question, PERSIAN_KNOWLEDGE, top_k=2)\n",
        "\n",
        "    # Step 2: Prepare context\n",
        "    if relevant_docs:\n",
        "        context = \"\\n\".join([f\"‚Ä¢ {doc}\" for doc in relevant_docs])\n",
        "        context_header = \"ÿßÿ∑ŸÑÿßÿπÿßÿ™ ŸÖÿ±ÿ™ÿ®ÿ∑:\\n\\n\"\n",
        "    else:\n",
        "        context = \"ÿßÿ∑ŸÑÿßÿπÿßÿ™ ŸÖÿ±ÿ™ÿ®ÿ∑€å €åÿßŸÅÿ™ ŸÜÿ¥ÿØ.\"\n",
        "        context_header = \"\"\n",
        "\n",
        "    # Step 3: Create prompt\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "ÿ¥ŸÖÿß €å⁄© ÿØÿ≥ÿ™€åÿßÿ± ŸÅÿßÿ±ÿ≥€å ŸáŸàÿ¥ŸÖŸÜÿØ Ÿáÿ≥ÿ™€åÿØ.\n",
        "\n",
        "{context_header}{context}\n",
        "\n",
        "ŸÑÿ∑ŸÅÿßŸã ÿ®ÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ®ÿßŸÑÿß ÿ®Ÿá ÿ≥ŸàÿßŸÑ Ÿæÿßÿ≥ÿÆ ÿØŸá€åÿØ.<|im_end|>\n",
        "\n",
        "<|im_start|>user\n",
        "{question}<|im_end|>\n",
        "\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    # Step 4: Generate answer\n",
        "    try:\n",
        "        if use_model and model_loaded:\n",
        "            # Tokenize prompt\n",
        "            inputs = tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=1024\n",
        "            )\n",
        "\n",
        "            # Generate response\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=150,           # Limit response length\n",
        "                    temperature=0.7,              # Creativity level\n",
        "                    do_sample=True,               # Enable sampling\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    repetition_penalty=1.1        # Reduce repetition\n",
        "                )\n",
        "\n",
        "            # Decode and clean response\n",
        "            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract assistant's response\n",
        "            if \"<|im_start|>assistant\" in full_response:\n",
        "                answer = full_response.split(\"<|im_start|>assistant\")[-1].strip()\n",
        "            else:\n",
        "                answer = tokenizer.decode(\n",
        "                    outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
        "                    skip_special_tokens=True\n",
        "                )\n",
        "\n",
        "            # Remove any remaining special tokens\n",
        "            answer = answer.split(\"<|im_end|>\")[0].strip()\n",
        "\n",
        "        else:\n",
        "            # Fallback response\n",
        "            if relevant_docs:\n",
        "                answer = f\"ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ÿßÿ∑ŸÑÿßÿπÿßÿ™: {relevant_docs[0][:80]}...\"\n",
        "            else:\n",
        "                answer = \"ŸÖÿ™ÿ£ÿ≥ŸÅÿßŸÜŸá ÿßÿ∑ŸÑÿßÿπÿßÿ™ ⁄©ÿßŸÅ€å ÿ®ÿ±ÿß€å Ÿæÿßÿ≥ÿÆ ÿ®Ÿá ÿß€åŸÜ ÿ≥ŸàÿßŸÑ ŸÜÿØÿßÿ±ŸÖ.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Generation error: {str(e)[:50]}\")\n",
        "        answer = \"ÿÆÿ∑ÿß ÿØÿ± ÿ™ŸàŸÑ€åÿØ Ÿæÿßÿ≥ÿÆ. ŸÑÿ∑ŸÅÿßŸã ÿØŸàÿ®ÿßÿ±Ÿá ÿ™ŸÑÿßÿ¥ ⁄©ŸÜ€åÿØ.\"\n",
        "\n",
        "    return answer.strip()\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 8: TEST THE RAG SYSTEM\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Test the RAG system with predefined Persian questions.\n",
        "Demonstrates the system's capabilities.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß™ TESTING PERSIAN RAG SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test questions in Persian\n",
        "test_questions = [\n",
        "    \"ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ⁄Ü€åÿ≥ÿ™ÿü\",\n",
        "    \"€åÿßÿØ⁄Ø€åÿ±€å ŸÖÿßÿ¥€åŸÜ ⁄ÜŸá ÿßŸÜŸàÿßÿπ€å ÿØÿßÿ±ÿØÿü\",\n",
        "    \"Ÿæÿ±ÿØÿßÿ≤ÿ¥ ÿ≤ÿ®ÿßŸÜ ÿ∑ÿ®€åÿπ€å ⁄Ü€åÿ≥ÿ™ÿü\",\n",
        "    \"RAG ⁄Ü⁄ØŸàŸÜŸá ⁄©ÿßÿ± ŸÖ€å‚Äå⁄©ŸÜÿØÿü\",\n",
        "    \"ÿ™ÿ±ŸÜÿ≥ŸÅŸàÿ±ŸÖÿ± ⁄Ü€åÿ≥ÿ™ÿü\"\n",
        "]\n",
        "\n",
        "# Run tests\n",
        "results = []\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{i}. ‚ùì Question: {question}\")\n",
        "\n",
        "    # Retrieve relevant documents\n",
        "    relevant_docs = retrieve_relevant_documents(question, PERSIAN_KNOWLEDGE)\n",
        "    print(f\"   üìö Relevant documents found: {len(relevant_docs)}\")\n",
        "\n",
        "    # Generate answer\n",
        "    answer = generate_rag_answer(question, use_model=model_loaded)\n",
        "\n",
        "    # Display answer (truncate if too long)\n",
        "    display_answer = answer[:120] + \"...\" if len(answer) > 120 else answer\n",
        "    print(f\"   üí¨ Answer: {display_answer}\")\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"docs_found\": len(relevant_docs),\n",
        "        \"model_used\": model_loaded\n",
        "    })\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 9: SYSTEM SUMMARY\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Display summary of the RAG system performance.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä SYSTEM SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "successful_tests = len([r for r in results if r[\"answer\"] and not r[\"answer\"].startswith(\"ÿÆÿ∑ÿß\")])\n",
        "total_questions = len(results)\n",
        "\n",
        "print(f\"‚úÖ Tests completed: {successful_tests}/{total_questions}\")\n",
        "print(f\"ü§ñ Real model used: {'Yes' if model_loaded else 'No (fallback mode)'}\")\n",
        "print(f\"üìö Knowledge base size: {len(PERSIAN_KNOWLEDGE)} documents\")\n",
        "print(f\"üîß Environment: PyTorch {torch.__version__} (CPU only)\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 10: USAGE EXAMPLE\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Example of how to use the RAG system with new questions.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí° HOW TO USE THE SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "To ask a new question in Persian:\n",
        "\n",
        "# Method 1: Simple retrieval + generation\n",
        "question = \"ÿ≥ŸàÿßŸÑ ŸÅÿßÿ±ÿ≥€å ÿ¥ŸÖÿß\"\n",
        "answer = generate_rag_answer(question, use_model=True)\n",
        "print(f\"ÿ≥ŸàÿßŸÑ: {question}\")\n",
        "print(f\"Ÿæÿßÿ≥ÿÆ: {answer}\")\n",
        "\n",
        "# Method 2: Just retrieval\n",
        "relevant = retrieve_relevant_documents(question, PERSIAN_KNOWLEDGE)\n",
        "print(f\"ŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ŸÖÿ±ÿ™ÿ®ÿ∑: {relevant}\")\n",
        "\"\"\")\n",
        "\n",
        "# ===========================================================\n",
        "# SECTION 11: FINAL TEST\n",
        "# ===========================================================\n",
        "\"\"\"\n",
        "Final test with a new question to demonstrate functionality.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ FINAL DEMONSTRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test a new question\n",
        "new_question = \"ÿ™ŸÅÿßŸàÿ™ ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å Ÿà €åÿßÿØ⁄Ø€åÿ±€å ŸÖÿßÿ¥€åŸÜ ⁄Ü€åÿ≥ÿ™ÿü\"\n",
        "print(f\"New question: {new_question}\")\n",
        "\n",
        "final_answer = generate_rag_answer(new_question, use_model=model_loaded)\n",
        "print(f\"\\nAnswer: {final_answer}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ PERSIAN RAG SYSTEM READY FOR USE!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "DFmzzNbNNt2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}