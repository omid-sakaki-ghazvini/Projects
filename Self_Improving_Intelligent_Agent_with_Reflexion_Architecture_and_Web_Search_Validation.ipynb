{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Install required packages ---\n",
        "!pip install -q google-generativeai duckduckgo-search"
      ],
      "metadata": {
        "id": "EVZIoFHkU-ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "import time\n",
        "from typing import Dict, List, TypedDict, Optional\n",
        "from datetime import datetime\n",
        "from duckduckgo_search import DDGS\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "4IQaYcQ3VEsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Gemini API Setup ---\n",
        "def setup_gemini_model():\n",
        "    \"\"\"\n",
        "    Configure Gemini model for Google Colab.\n",
        "    API key must be stored in Colab's secrets as 'GEMINI_API_KEY'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('GEMINI_API_KEY')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY not found in Colab secrets.\")\n",
        "    except Exception:\n",
        "        # Fallback: Prompt user for API key\n",
        "        api_key = input(\"Please enter your Gemini API key: \").strip()\n",
        "        if not api_key:\n",
        "            raise ValueError(\"No Gemini API key provided.\")\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 1,\n",
        "        \"top_k\": 1,\n",
        "        \"max_output_tokens\": 2048,\n",
        "    }\n",
        "    safety_settings = [\n",
        "        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    ]\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-1.5-flash\",\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "aOxYE3b7VKcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Initialize Model ---\n",
        "model = setup_gemini_model()"
      ],
      "metadata": {
        "id": "pOZi9rWNVMps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Agent State ---\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    initial_response: str\n",
        "    critiques: List[str]\n",
        "    refined_responses: List[str]\n",
        "    search_results: List[Dict]\n",
        "    current_response: str\n",
        "    iteration: int\n",
        "    is_satisfactory: bool\n",
        "    max_iterations: int\n",
        "    final_response: Optional[str]\n",
        "    completed_at: Optional[str]\n",
        "    total_iterations: Optional[int]"
      ],
      "metadata": {
        "id": "Jk1Bz33iVOnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Web Search Tool ---\n",
        "def web_search(query: str, max_results: int = 5) -> List[Dict]:\n",
        "    \"\"\"Perform web search using DuckDuckGo.\"\"\"\n",
        "    try:\n",
        "        with DDGS() as ddgs:\n",
        "            results = list(ddgs.text(query, max_results=max_results))\n",
        "            return results\n",
        "    except Exception as e:\n",
        "        print(f\"[Web Search Error]: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "Lmng3VbwVQgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Gemini Response Generation ---\n",
        "def generate_response(prompt: str, model=model) -> str:\n",
        "    \"\"\"Generate response using Gemini model.\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"[Gemini Error]: {str(e)}\")\n",
        "        return f\"Error generating response: {str(e)}\""
      ],
      "metadata": {
        "id": "tFOA_ROYVSat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Self-Critique ---\n",
        "def self_critique(question: str, response: str, search_results: Optional[List[Dict]] = None) -> str:\n",
        "    \"\"\"Critique the model's response.\"\"\"\n",
        "    critique_prompt = f\"\"\"\n",
        "You are a critical reviewer. Analyze the response to the question and provide constructive criticism.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "RESPONSE: {response}\n",
        "\n",
        "{\"ADDITIONAL CONTEXT FROM SEARCH: \" + json.dumps(search_results, ensure_ascii=False) if search_results else \"\"}\n",
        "\n",
        "Please critique:\n",
        "1. Factual accuracy and errors\n",
        "2. Completeness\n",
        "3. Clarity and structure\n",
        "4. Missing information\n",
        "5. Suggestions for improvement\n",
        "\n",
        "CRITIQUE:\n",
        "\"\"\"\n",
        "    return generate_response(critique_prompt)"
      ],
      "metadata": {
        "id": "oqO-j6XLVU4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Response Refinement ---\n",
        "def refine_response(question: str, current_response: str, critique: str, search_results: Optional[List[Dict]] = None) -> str:\n",
        "    \"\"\"Refine response based on critique and web search.\"\"\"\n",
        "    refinement_prompt = f\"\"\"\n",
        "Refine your response to the question using the provided critique and search results.\n",
        "\n",
        "QUESTION: {question}\n",
        "CURRENT RESPONSE: {current_response}\n",
        "CRITIQUE: {critique}\n",
        "{\"SEARCH RESULTS FOR VALIDATION: \" + json.dumps(search_results, ensure_ascii=False) if search_results else \"\"}\n",
        "\n",
        "Instructions:\n",
        "- Address all critique points\n",
        "- Incorporate relevant facts from search results\n",
        "- Maintain clarity, professionalism, accuracy, and completeness\n",
        "\n",
        "REFINED RESPONSE:\n",
        "\"\"\"\n",
        "    return generate_response(refinement_prompt)"
      ],
      "metadata": {
        "id": "80Bn_gAoVXOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Satisfaction Evaluation ---\n",
        "def evaluate_satisfaction(question: str, response: str, critique_history: List[str]) -> bool:\n",
        "    \"\"\"Evaluate if the response is satisfactory.\"\"\"\n",
        "    evaluation_prompt = f\"\"\"\n",
        "Evaluate if the following response satisfactorily answers the question.\n",
        "\n",
        "QUESTION: {question}\n",
        "RESPONSE: {response}\n",
        "CRITIQUE HISTORY: {json.dumps(critique_history, ensure_ascii=False)}\n",
        "\n",
        "Assess:\n",
        "1. Does it fully answer the question?\n",
        "2. Is it factually accurate?\n",
        "3. Is it well-structured and clear?\n",
        "4. Are previous critique points resolved?\n",
        "\n",
        "Answer with ONLY \"YES\" or \"NO\" and a brief explanation.\n",
        "\n",
        "EVALUATION:\n",
        "\"\"\"\n",
        "    evaluation = generate_response(evaluation_prompt)\n",
        "    return \"YES\" in evaluation.upper() and (\"thorough\" in evaluation.lower() or \"complete\" in evaluation.lower())"
      ],
      "metadata": {
        "id": "sY4v8Ci2VZ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Search Query Generation ---\n",
        "def generate_search_queries(question: str, response: str, critique: str) -> List[str]:\n",
        "    \"\"\"Generate search queries for fact-checking.\"\"\"\n",
        "    search_prompt = f\"\"\"\n",
        "Based on the question, response, and critique, generate 3 search queries to fact-check and improve the response.\n",
        "\n",
        "QUESTION: {question}\n",
        "RESPONSE: {response}\n",
        "CRITIQUE: {critique}\n",
        "\n",
        "Return ONLY the queries as a JSON list: [\"query1\", \"query2\", \"query3\"]\n",
        "\n",
        "SEARCH QUERIES:\n",
        "\"\"\"\n",
        "    queries_text = generate_response(search_prompt)\n",
        "    try:\n",
        "        queries = json.loads(queries_text)\n",
        "    except Exception:\n",
        "        queries = re.findall(r'\"([^\"]*)\"', queries_text)\n",
        "        if not queries:\n",
        "            queries = [f\"fact check: {question}\", f\"latest research: {question}\", question]\n",
        "    return queries[:3]"
      ],
      "metadata": {
        "id": "dUky_K0TVdAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Reflexion Workflow ---\n",
        "def reflexion_workflow(question: str, max_iterations: int = 3) -> Dict:\n",
        "    \"\"\"Run the Reflexion loop to answer a question.\"\"\"\n",
        "    state = AgentState(\n",
        "        question=question,\n",
        "        initial_response=\"\",\n",
        "        critiques=[],\n",
        "        refined_responses=[],\n",
        "        search_results=[],\n",
        "        current_response=\"\",\n",
        "        iteration=0,\n",
        "        is_satisfactory=False,\n",
        "        max_iterations=max_iterations,\n",
        "        final_response=None,\n",
        "        completed_at=None,\n",
        "        total_iterations=None\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüß† Question: {question}\")\n",
        "    print(\"=\" * 60)\n",
        "    # Step 1: Initial Response\n",
        "    print(\"üîÑ Generating initial response...\")\n",
        "    state['current_response'] = generate_response(f\"Please provide a comprehensive answer to the following question: {question}\")\n",
        "    state['initial_response'] = state['current_response']\n",
        "    state['refined_responses'].append(state['current_response'])\n",
        "\n",
        "    # Reflexion Loop\n",
        "    while state['iteration'] < state['max_iterations'] and not state['is_satisfactory']:\n",
        "        state['iteration'] += 1\n",
        "        print(f\"\\nüîÑ Iteration {state['iteration']} / {state['max_iterations']}\")\n",
        "\n",
        "        # Step 2: Critique\n",
        "        print(\"üîç Critiquing response...\")\n",
        "        critique = self_critique(state['question'], state['current_response'], state['search_results'] if state['search_results'] else None)\n",
        "        state['critiques'].append(critique)\n",
        "\n",
        "        # Step 3: Search & Validation\n",
        "        if state['iteration'] > 1 or \"fact\" in critique.lower() or \"accurate\" in critique.lower():\n",
        "            print(\"üåê Generating search queries...\")\n",
        "            queries = generate_search_queries(state['question'], state['current_response'], critique)\n",
        "            print(f\"Search queries: {queries}\")\n",
        "            new_search_results = []\n",
        "            for query in queries:\n",
        "                results = web_search(query, max_results=2)\n",
        "                new_search_results.extend(results)\n",
        "                time.sleep(1)  # Polite delay\n",
        "            state['search_results'].extend(new_search_results)\n",
        "            print(f\"üîé {len(new_search_results)} new search results added.\")\n",
        "\n",
        "        # Step 4: Refine Response\n",
        "        print(\"üõ†Ô∏è Refining response...\")\n",
        "        refined_response = refine_response(\n",
        "            state['question'],\n",
        "            state['current_response'],\n",
        "            critique,\n",
        "            state['search_results'] if state['search_results'] else None\n",
        "        )\n",
        "        state['current_response'] = refined_response\n",
        "        state['refined_responses'].append(refined_response)\n",
        "\n",
        "        # Step 5: Evaluate Satisfaction\n",
        "        print(\"‚úÖ Evaluating response quality...\")\n",
        "        state['is_satisfactory'] = evaluate_satisfaction(\n",
        "            state['question'],\n",
        "            state['current_response'],\n",
        "            state['critiques']\n",
        "        )\n",
        "        print(\"üéâ Satisfactory response!\" if state['is_satisfactory'] else \"‚ö†Ô∏è Needs improvement.\")\n",
        "\n",
        "    # Finalize State\n",
        "    state['final_response'] = state['current_response']\n",
        "    state['completed_at'] = datetime.now().isoformat()\n",
        "    state['total_iterations'] = state['iteration']\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "1eZBghoUVg9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Display Results ---\n",
        "def format_response(response: Dict) -> str:\n",
        "    \"\"\"Formatted output for display.\"\"\"\n",
        "    output = [\n",
        "        f\"QUESTION: {response['question']}\",\n",
        "        \"=\" * 60,\n",
        "        f\"FINAL RESPONSE (after {response['total_iterations']} iterations):\",\n",
        "        \"-\" * 40,\n",
        "        textwrap.fill(response['final_response'], width=80),\n",
        "        \"\\nCRITIQUES:\",\n",
        "        \"-\" * 40\n",
        "    ]\n",
        "    for i, critique in enumerate(response['critiques'], 1):\n",
        "        output.append(f\"Critique {i}: {textwrap.shorten(critique, width=100, placeholder='...')}\")\n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "def save_results(results: Dict, filename: str = None):\n",
        "    \"\"\"Save results to a JSON file.\"\"\"\n",
        "    if not filename:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"reflexion_agent_results_{timestamp}.json\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Results saved to {filename}\")"
      ],
      "metadata": {
        "id": "NeL10-T2Vjc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Demonstration ---\n",
        "def demonstrate_workflow():\n",
        "    \"\"\"Run demo with example questions.\"\"\"\n",
        "    example_questions = [\n",
        "        \"What are the latest advancements in quantum computing for 2024?\",\n",
        "        \"Explain the implications of CRISPR gene editing technology on modern medicine.\",\n",
        "        \"How is artificial intelligence being used in climate change research?\"\n",
        "    ]\n",
        "    print(\"\\nü§ñ Reflexion AI Agent Demo\")\n",
        "    print(\"=\" * 60)\n",
        "    for i, question in enumerate(example_questions, 1):\n",
        "        print(f\"\\nExample {i}: {question}\")\n",
        "        print(\"-\" * 40)\n",
        "        result = reflexion_workflow(question, max_iterations=2)\n",
        "        print(f\"\\n‚úÖ Final response after {result['total_iterations']} iterations:\\n{'='*50}\")\n",
        "        print(textwrap.fill(result['final_response'], width=80))\n",
        "        initial_len = len(result['initial_response'])\n",
        "        final_len = len(result['final_response'])\n",
        "        print(f\"\\nüìä Improvement: {initial_len} ‚ûî {final_len} characters ({final_len/initial_len:.1f}x detail)\")\n",
        "        if result['search_results']:\n",
        "            print(f\"üîç Used {len(result['search_results'])} search results for validation\")\n",
        "        print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "id": "qUp_b4xBVlvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Interactive Mode ---\n",
        "def interactive_mode():\n",
        "    \"\"\"Interactive Q&A mode.\"\"\"\n",
        "    print(\"\\nü§ñ Reflexion AI Agent - Interactive Mode\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    print(\"=\" * 50)\n",
        "    while True:\n",
        "        question = input(\"\\nüìù Enter your question: \").strip()\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        if not question:\n",
        "            continue\n",
        "        print(\"Processing your question...\")\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = reflexion_workflow(question, max_iterations=2)\n",
        "            end_time = time.time()\n",
        "            print(f\"\\n‚úÖ Completed in {end_time - start_time:.1f} seconds\")\n",
        "            print(format_response(result))\n",
        "            save = input(\"\\nüíæ Save these results? (y/n): \").lower()\n",
        "            if save.startswith('y'):\n",
        "                save_results(result)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "0IWaXg3cVo0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Uncomment below to run demo or interactive mode ---\n",
        "demonstrate_workflow()\n",
        "#interactive_mode()"
      ],
      "metadata": {
        "id": "JIaGeyzaWGyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}