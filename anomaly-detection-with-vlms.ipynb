{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3209332,"sourceType":"datasetVersion","datasetId":1946896}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **INSTALLATION & IMPORTS**","metadata":{}},{"cell_type":"code","source":"print(\"Installing required packages...\")\n!pip install git+https://github.com/openai/CLIP.git -q\n!pip install imageio[ffmpeg] -q\n\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image, ImageDraw, ImageFont\nimport imageio\nimport warnings\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import Dataset, DataLoader\nimport clip\n\nwarnings.filterwarnings(\"ignore\")\n\n# Set seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CONFIGURATION**","metadata":{}},{"cell_type":"code","source":"class Config:\n    \"\"\"Centralized configuration for easy tuning and reproducibility.\"\"\"\n    \n    # Dataset path (Kaggle-specific)\n    DATA_PATH = \"/kaggle/input/mvtec-ad\"\n    \n    # CLIP model selection\n    # ViT-B/32: faster, lower memory\n    # ViT-L/14 or ViT-L/14@336px: higher accuracy (use if GPU memory allows)\n    CLIP_MODEL = \"ViT-B/32\"\n    \n    # Training/Evaluation settings\n    BATCH_SIZE = 32\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Visualization settings\n    NUM_GIF_FRAMES = 40\n    GIF_OUTPUT_PATH = \"/kaggle/working/anomaly_detection_comparison.gif\"\n    \n    # MVTec AD categories\n    CATEGORIES = [\n        \"bottle\", \"cable\", \"capsule\", \"carpet\", \"grid\",\n        \"hazelnut\", \"leather\", \"metal_nut\", \"pill\", \"screw\",\n        \"tile\", \"toothbrush\", \"transistor\", \"wood\", \"zipper\"\n    ]\n    \n    # Method names for reporting\n    METHODS = [\"AnomalyCLIP\", \"WinCLIP\", \"PA-CLIP\", \"AA/AF-CLIP\"]\n    \n    # Shared prompt templates\n    NORMAL_TEMPLATES = [\"a photo of a {}\", \"a good photo of a {}\", \"a cropped photo of a {}\"]\n    ANOMALY_TEMPLATES = [\"a photo of a {}\", \"a bad photo of a {}\", \"a defective {}\"]\n\nconfig = Config()\n\nprint(f\"Configuration loaded.\")\nprint(f\"Device: {config.DEVICE}\")\nprint(f\"CLIP Model: {config.CLIP_MODEL}\")\nprint(f\"Categories: {len(config.CATEGORIES)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DATASET**","metadata":{}},{"cell_type":"code","source":"class MVTecADDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for MVTec Anomaly Detection.\n    Loads test images (good + all defect types) with binary labels.\n    \"\"\"\n    \n    def __init__(self, root_path: str, category: str, split: str = \"test\", transform=None):\n        self.root_path = root_path\n        self.category = category\n        self.split = split\n        self.transform = transform\n        \n        self.image_paths = []\n        self.labels = []  # 0: normal (good), 1: anomalous\n        \n        base_path = os.path.join(root_path, category, split)\n        \n        if not os.path.exists(base_path):\n            raise ValueError(f\"Path does not exist: {base_path}\")\n        \n        # Load normal ('good') images\n        good_path = os.path.join(base_path, \"good\")\n        if os.path.exists(good_path):\n            for img_name in sorted(os.listdir(good_path)):\n                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    img_path = os.path.join(good_path, img_name)\n                    self.image_paths.append(img_path)\n                    self.labels.append(0)\n        \n        # Load all anomalous defect types\n        for defect_type in os.listdir(base_path):\n            if defect_type != \"good\" and os.path.isdir(os.path.join(base_path, defect_type)):\n                defect_path = os.path.join(base_path, defect_type)\n                for img_name in sorted(os.listdir(defect_path)):\n                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        img_path = os.path.join(defect_path, img_name)\n                        self.image_paths.append(img_path)\n                        self.labels.append(1)\n        \n        print(f\"{category} | {split} | Loaded {len(self.image_paths)} images \"\n              f\"({sum(self.labels)} anomalous, {len(self.labels) - sum(self.labels)} normal)\")\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label, img_path","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **UTILITIES**","metadata":{}},{"cell_type":"code","source":"def get_text_features(model, texts: list) -> torch.Tensor:\n    \"\"\"Encode list of text prompts and return normalized features.\"\"\"\n    tokens = clip.tokenize(texts).to(config.DEVICE)\n    with torch.no_grad():\n        text_features = model.encode_text(tokens)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n    return text_features\n\ndef compute_anomaly_score(image_features: torch.Tensor,\n                          normal_features: torch.Tensor,\n                          anomaly_features: torch.Tensor) -> np.ndarray:\n    \"\"\"\n    Compute anomaly score as average of:\n        (1 - similarity to normal) + similarity to anomaly\n    Higher score → more anomalous\n    \"\"\"\n    sim_normal = (image_features @ normal_features.T).squeeze(1)\n    sim_anomaly = (image_features @ anomaly_features.T).squeeze(1)\n    scores = (1 - sim_normal + sim_anomaly) / 2.0\n    return scores.cpu().numpy()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **ANOMALY DETECTION METHODS**","metadata":{}},{"cell_type":"code","source":"class BaseAnomalyDetector:\n    \"\"\"Base class for all CLIP-based zero-shot detectors.\"\"\"\n    \n    def __init__(self, clip_model, preprocess):\n        self.clip_model = clip_model\n        self.preprocess = preprocess\n    \n    def detect(self, images: torch.Tensor, category: str) -> np.ndarray:\n        raise NotImplementedError\n\nclass AnomalyCLIPDetector(BaseAnomalyDetector):\n    \"\"\"Simple AnomalyCLIP-style prompting.\"\"\"\n    \n    def detect(self, images: torch.Tensor, category: str) -> np.ndarray:\n        normal_texts = [t.format(f\"good {category}\") for t in config.NORMAL_TEMPLATES]\n        anomaly_texts = [t.format(f\"defective {category}\") for t in config.ANOMALY_TEMPLATES]\n        \n        normal_feat = get_text_features(self.clip_model, normal_texts).mean(0, keepdim=True)\n        anomaly_feat = get_text_features(self.clip_model, anomaly_texts).mean(0, keepdim=True)\n        \n        with torch.no_grad():\n            img_feat = self.clip_model.encode_image(images)\n            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n        \n        return compute_anomaly_score(img_feat, normal_feat, anomaly_feat)\n\nclass WinCLIPDetector(BaseAnomalyDetector):\n    \"\"\"WinCLIP-style: rich multi-state prompts.\"\"\"\n    \n    def detect(self, images: torch.Tensor, category: str) -> np.ndarray:\n        normal_states = [\"good\", \"clean\", \"perfect\", \"flawless\", \"undamaged\"]\n        anomaly_states = [\"broken\", \"damaged\", \"defective\", \"scratched\", \"cracked\"]\n        \n        normal_texts = [t.format(f\"{state} {category}\")\n                       for t in config.NORMAL_TEMPLATES for state in normal_states]\n        anomaly_texts = [t.format(f\"{state} {category}\")\n                        for t in config.ANOMALY_TEMPLATES for state in anomaly_states]\n        \n        normal_feat = get_text_features(self.clip_model, normal_texts).mean(0, keepdim=True)\n        anomaly_feat = get_text_features(self.clip_model, anomaly_texts).mean(0, keepdim=True)\n        \n        with torch.no_grad():\n            img_feat = self.clip_model.encode_image(images)\n            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n        \n        return compute_anomaly_score(img_feat, normal_feat, anomaly_feat)\n\nclass PACLIPDetector(BaseAnomalyDetector):\n    \"\"\"Pseudo-Anomaly Aware CLIP: enhances normal representation with pseudo-anomalies.\"\"\"\n    \n    def detect(self, images: torch.Tensor, category: str) -> np.ndarray:\n        normal_base = [\"a photo of a clean \" + category, \"a flawless \" + category]\n        pseudo_additions = [\"with noise\", \"with shadow\", \"slightly deformed\", \"with background clutter\"]\n        \n        pseudo_texts = [f\"{nt} {pa}\" for nt in normal_base for pa in pseudo_additions]\n        anomaly_texts = [\n            f\"a photo of a defective {category}\",\n            f\"a broken {category}\",\n            f\"a scratched {category}\"\n        ]\n        \n        normal_feat = get_text_features(self.clip_model, normal_base).mean(0, keepdim=True)\n        pseudo_feat = get_text_features(self.clip_model, pseudo_texts).mean(0, keepdim=True)\n        anomaly_feat = get_text_features(self.clip_model, anomaly_texts).mean(0, keepdim=True)\n        \n        # Enhanced normal representation\n        enhanced_normal = (normal_feat + pseudo_feat) / 2\n        \n        with torch.no_grad():\n            img_feat = self.clip_model.encode_image(images)\n            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n        \n        return compute_anomaly_score(img_feat, enhanced_normal, anomaly_feat)\n\nclass AAFCLIPDetector(BaseAnomalyDetector):\n    \"\"\"Anomaly-Aware / Anomaly-Focused CLIP with high-quality templates.\"\"\"\n    \n    def detect(self, images: torch.Tensor, category: str) -> np.ndarray:\n        templates = [\n            \"a high quality photo of a normal {}\",\n            \"a detailed photo of an undamaged {}\",\n            \"a clear photo of a perfect {}\",\n            \"a close-up of a flawless {}\"\n        ]\n        anomaly_states = [\"with defect\", \"with scratch\", \"with crack\",\n                         \"with contamination\", \"abnormal\", \"deformed\"]\n        \n        normal_texts = [t.format(category) for t in templates]\n        anomaly_texts = [t.format(category) + \" \" + s for t in templates for s in anomaly_states]\n        \n        normal_feat = get_text_features(self.clip_model, normal_texts).mean(0, keepdim=True)\n        anomaly_feat = get_text_features(self.clip_model, anomaly_texts).mean(0, keepdim=True)\n        \n        with torch.no_grad():\n            img_feat = self.clip_model.encode_image(images)\n            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n        \n        return compute_anomaly_score(img_feat, normal_feat, anomaly_feat)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **EVALUATION**","metadata":{}},{"cell_type":"code","source":"def evaluate_category(clip_model, preprocess, category: str, detectors: dict) -> dict:\n    \"\"\"Evaluate all methods on one MVTec AD category and return AUROC scores.\"\"\"\n    \n    print(f\"\\nEvaluating category: {category.upper()}\")\n    \n    dataset = MVTecADDataset(config.DATA_PATH, category, \"test\", transform=preprocess)\n    dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    results = {name: [] for name in detectors}\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels, _ in tqdm(dataloader, desc=\"Processing\", leave=False):\n            images = images.to(config.DEVICE)\n            all_labels.extend(labels.numpy())\n            \n            for name, detector in detectors.items():\n                scores = detector.detect(images, category)\n                results[name].extend(scores)\n    \n    # Compute AUROC\n    auroc_scores = {}\n    for name, scores in results.items():\n        auroc = roc_auc_score(all_labels, scores)\n        auroc_scores[name] = round(auroc, 4)\n        print(f\"{name:15} AUROC = {auroc:.4f}\")\n    \n    return auroc_scores","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **VISUALIZATION**","metadata":{}},{"cell_type":"code","source":"def plot_results(df: pd.DataFrame):\n    \"\"\"Generate professional plots: bar, box, heatmap.\"\"\"\n    \n    sns.set(style=\"whitegrid\", font_scale=1.2)\n    \n    # Bar plot per category\n    plt.figure(figsize=(16, 8))\n    df_melted = df.melt(id_vars=\"Category\", var_name=\"Method\", value_name=\"AUROC\")\n    sns.barplot(x=\"Category\", y=\"AUROC\", hue=\"Method\", data=df_melted, palette=\"viridis\")\n    plt.title(\"Zero-Shot Anomaly Detection AUROC per Category\", fontsize=16, fontweight=\"bold\")\n    plt.ylabel(\"AUROC\")\n    plt.ylim(0, 1.05)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.legend(title=\"Method\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n    plt.tight_layout()\n    plt.savefig(\"/kaggle/working/bar_plot.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    \n    # Box plot\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df.drop(columns=\"Category\"), palette=\"Set2\")\n    plt.title(\"AUROC Distribution Across Categories\", fontsize=16, fontweight=\"bold\")\n    plt.ylabel(\"AUROC\")\n    plt.ylim(0, 1.05)\n    plt.tight_layout()\n    plt.savefig(\"/kaggle/working/box_plot.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    \n    # Heatmap\n    plt.figure(figsize=(10, 12))\n    sns.heatmap(df.set_index(\"Category\"), annot=True, cmap=\"YlOrRd\", fmt=\".3f\",\n                linewidths=.5, cbar_kws={\"shrink\": 0.8})\n    plt.title(\"AUROC Heatmap\", fontsize=16, fontweight=\"bold\")\n    plt.tight_layout()\n    plt.savefig(\"/kaggle/working/heatmap.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\ndef create_comparison_gif(clip_model, preprocess):\n    \"\"\"Generate animated GIF comparing all methods on random test samples.\"\"\"\n    \n    print(\"\\nGenerating comparison GIF...\")\n    \n    detectors = {\n        \"AnomalyCLIP\": AnomalyCLIPDetector(clip_model, preprocess),\n        \"WinCLIP\": WinCLIPDetector(clip_model, preprocess),\n        \"PA-CLIP\": PACLIPDetector(clip_model, preprocess),\n        \"AA/AF-CLIP\": AAFCLIPDetector(clip_model, preprocess)\n    }\n    \n    # Collect random samples\n    samples = []\n    for cat in config.CATEGORIES:\n        dataset = MVTecADDataset(config.DATA_PATH, cat, \"test\", transform=None)\n        for i in range(min(3, len(dataset))):\n            samples.append((dataset.image_paths[i], cat, dataset.labels[i]))\n    \n    np.random.shuffle(samples)\n    selected = samples[:config.NUM_GIF_FRAMES]\n    \n    frames = []\n    for img_path, cat, label in tqdm(selected, desc=\"Creating frames\"):\n        pil_img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n        tensor_img = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(config.DEVICE)\n        \n        scores = {name: detector.detect(tensor_img, cat)[0] for name, detector in detectors.items()}\n        \n        frame = create_gif_frame(pil_img, scores, cat, label)\n        frames.append(np.array(frame))\n    \n    imageio.mimsave(config.GIF_OUTPUT_PATH, frames, fps=2, loop=0)\n    print(f\"GIF saved: {config.GIF_OUTPUT_PATH}\")\n\ndef create_gif_frame(pil_img: Image.Image, scores: dict, category: str, label: int) -> Image.Image:\n    \"\"\"Create one frame for the comparison GIF.\"\"\"\n    \n    w, h = pil_img.size\n    frame_w = w * 4 + 100\n    frame_h = h + 200\n    frame = Image.new(\"RGB\", (frame_w, frame_h), \"white\")\n    draw = ImageDraw.Draw(frame)\n    \n    try:\n        font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 24)\n        title_font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 32)\n    except:\n        font = ImageFont.load_default()\n        title_font = ImageFont.load_default()\n    \n    title = f\"Category: {category.upper()} | Ground Truth: {'ANOMALY' if label else 'NORMAL'}\"\n    draw.text((20, 20), title, fill=\"black\", font=title_font)\n    draw.text((20, 70), \"Zero-Shot CLIP-based Anomaly Detection Comparison\", fill=\"gray\", font=font)\n    \n    for i, (method, score) in enumerate(scores.items()):\n        img_copy = pil_img.copy()\n        border_color = \"red\" if score > 0.5 else \"green\"\n        bordered = Image.new(\"RGB\", (w + 12, h + 12), border_color)\n        bordered.paste(img_copy, (6, 6))\n        \n        d = ImageDraw.Draw(bordered)\n        text = f\"{method}\\nScore: {score:.3f}\\n{'ANOMALY' if score > 0.5 else 'NORMAL'}\"\n        y = 10\n        for line in text.split(\"\\n\"):\n            d.text((10, y), line, fill=\"white\", font=font, stroke_fill=\"black\", stroke_width=2)\n            y += 30\n        \n        frame.paste(bordered, (20 + i * (w + 20), 120))\n    \n    # Legend\n    y_legend = frame_h - 50\n    draw.rectangle([20, y_legend, 40, y_legend+20], fill=\"green\")\n    draw.text((50, y_legend), \"Predicted Normal\", fill=\"black\", font=font)\n    draw.rectangle([220, y_legend, 240, y_legend+20], fill=\"red\")\n    draw.text((250, y_legend), \"Predicted Anomaly\", fill=\"black\", font=font)\n    \n    return frame","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **MAIN EXECUTION**","metadata":{}},{"cell_type":"code","source":"def main():\n    print(\"\\n\" + \"=\"*80)\n    print(\"ZERO-SHOT ANOMALY DETECTION ON MVTEC AD - PROFESSIONAL IMPLEMENTATION\")\n    print(\"=\"*80)\n    \n    # Load CLIP\n    print(\"\\nLoading CLIP model...\")\n    clip_model, preprocess = clip.load(config.CLIP_MODEL, device=config.DEVICE)\n    clip_model.eval()\n    print(f\"CLIP {config.CLIP_MODEL} loaded on {config.DEVICE}\")\n    \n    # Initialize detectors\n    detectors = {\n        \"AnomalyCLIP\": AnomalyCLIPDetector(clip_model, preprocess),\n        \"WinCLIP\": WinCLIPDetector(clip_model, preprocess),\n        \"PA-CLIP\": PACLIPDetector(clip_model, preprocess),\n        \"AA/AF-CLIP\": AAFCLIPDetector(clip_model, preprocess)\n    }\n    \n    # Evaluate all categories\n    all_results = []\n    for category in config.CATEGORIES:\n        result = evaluate_category(clip_model, preprocess, category, detectors)\n        result[\"Category\"] = category\n        all_results.append(result)\n    \n    # Create DataFrame\n    results_df = pd.DataFrame(all_results)\n    mean_row = pd.DataFrame([{\n        \"Category\": \"MEAN\",\n        **{col: results_df[col].mean() for col in config.METHODS}\n    }])\n    results_df = pd.concat([results_df, mean_row], ignore_index=True)\n    \n    # Save and display results\n    csv_path = \"/kaggle/working/mvtec_ad_zero_shot_results.csv\"\n    results_df.to_csv(csv_path, index=False)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"FINAL RESULTS (Image-level AUROC)\")\n    print(\"=\"*80)\n    print(results_df.round(4).to_string(index=False))\n    print(f\"\\nResults saved to: {csv_path}\")\n    \n    # Visualizations\n    plot_results(results_df.iloc[:-1])  # Exclude mean row from plots\n    \n    # GIF\n    create_comparison_gif(clip_model, preprocess)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n    print(\"=\"*80)\n    print(\"Outputs:\")\n    print(f\"  • CSV Results: {csv_path}\")\n    print(f\"  • GIF Comparison: {config.GIF_OUTPUT_PATH}\")\n    print(\"  • Plots: bar_plot.png, box_plot.png, heatmap.png\")\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **RUN**","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    if not os.path.exists(config.DATA_PATH):\n        print(f\"ERROR: Dataset not found at {config.DATA_PATH}\")\n        print(\"Please add the 'MVTec AD' dataset to your Kaggle notebook:\")\n        print(\"   → Add Data → Search 'mvtec ad' → Add\")\n    else:\n        main()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-24T22:28:49.279Z"}},"outputs":[],"execution_count":null}]}