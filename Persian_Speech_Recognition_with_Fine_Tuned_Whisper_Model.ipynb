{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1: Install required packages**"
      ],
      "metadata": {
        "id": "7N5gwnQ6L8uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.45.2 datasets==3.0.1 evaluate==0.4.3 jiwer==3.0.4 soundfile==0.12.1 librosa==0.10.2 accelerate==0.34.2 --quiet"
      ],
      "metadata": {
        "id": "eVZa8f-VMDy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2: Import libraries**"
      ],
      "metadata": {
        "id": "wzeIb5zmMGoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperTokenizer, WhisperFeatureExtractor\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "torch.cuda.empty_cache()  # Clear CUDA memory"
      ],
      "metadata": {
        "id": "nu8zpz1nMNJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3: Download and prepare data (small size for free Colab)**"
      ],
      "metadata": {
        "id": "YJ--cmVwMQUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fa\", split=\"train[:500]\")\n",
        "eval_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fa\", split=\"validation[:50]\")\n",
        "\n",
        "def normalize_audio(example):\n",
        "    audio = example[\"audio\"][\"array\"]\n",
        "    sr = example[\"audio\"][\"sampling_rate\"]\n",
        "    if sr != 16000:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "    example[\"audio\"][\"array\"] = audio\n",
        "    example[\"audio\"][\"sampling_rate\"] = 16000\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(normalize_audio)\n",
        "eval_dataset = eval_dataset.map(normalize_audio)\n",
        "\n",
        "def prepare_text(example):\n",
        "    example[\"sentence\"] = example[\"sentence\"].strip()\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(prepare_text)\n",
        "eval_dataset = eval_dataset.map(prepare_text)\n",
        "\n",
        "# Filter invalid examples\n",
        "def filter_invalid(example):\n",
        "    return len(example[\"audio\"][\"array\"]) > 0 and len(example[\"sentence\"]) > 0\n",
        "\n",
        "train_dataset = train_dataset.filter(filter_invalid)\n",
        "eval_dataset = eval_dataset.filter(filter_invalid)\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"fa\", task=\"transcribe\")\n",
        "\n",
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "    features = feature_extractor(audio[\"array\"], sampling_rate=16000).input_features[0]\n",
        "    labels = tokenizer(example[\"sentence\"], add_special_tokens=True).input_ids\n",
        "    example[\"input_features\"] = features\n",
        "    example[\"labels\"] = labels\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names)\n",
        "eval_dataset = eval_dataset.map(prepare_dataset, remove_columns=eval_dataset.column_names)\n",
        "\n",
        "# Debug: Inspect dataset\n",
        "print(\"Train sample shape:\", np.array(train_dataset[0][\"input_features\"]).shape, \"labels len:\", len(train_dataset[0][\"labels\"]))\n",
        "print(\"Eval sample shape:\", np.array(eval_dataset[0][\"input_features\"]).shape, \"labels len:\", len(eval_dataset[0][\"labels\"]))"
      ],
      "metadata": {
        "id": "uR7PlGu7MWPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4: Load Whisper-Tiny model**"
      ],
      "metadata": {
        "id": "kJLKZZ8tMiOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
        "model.generation_config.language = \"fa\"\n",
        "model.generation_config.task = \"transcribe\"\n",
        "model.generation_config.forced_decoder_ids = tokenizer.get_decoder_prompt_ids(language=\"fa\", task=\"transcribe\")\n",
        "model.generation_config.max_length = 448\n",
        "model.to(device)\n",
        "print(\"‚úÖ Model loaded.\")"
      ],
      "metadata": {
        "id": "Hec5cHhRMlGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5: Define WER metric**"
      ],
      "metadata": {
        "id": "dcGzROPTMnhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Handle tuple case for pred_ids\n",
        "    if isinstance(pred_ids, tuple):\n",
        "        pred_ids = pred_ids[0]\n",
        "\n",
        "    if not isinstance(pred_ids, torch.Tensor):\n",
        "        pred_ids = torch.tensor(pred_ids)\n",
        "    if pred_ids.ndim == 3:\n",
        "        pred_ids = pred_ids.argmax(dim=-1)\n",
        "\n",
        "    label_ids = torch.tensor(label_ids)\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Debug: Print sample predictions\n",
        "    for i in range(min(3, len(pred_str))):\n",
        "        print(f\"Prediction {i}: {pred_str[i]}\")\n",
        "        print(f\"Reference {i}: {label_str[i]}\")\n",
        "\n",
        "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "-E4dB8kXMrps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6: Training arguments and data collator (optimized for free Colab)**"
      ],
      "metadata": {
        "id": "fRuOlmrBMvAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./whisper-fa-finetuned\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,             # Minimum value to prevent memory errors\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,             # Effective for maintaining larger batch size\n",
        "    num_train_epochs=4,                        # Moderate number of epochs\n",
        "    learning_rate=1e-5,                        # Reduced lr for stability\n",
        "    warmup_steps=100,                          # Add warmup to prevent divergence\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        "    save_steps=40,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=20,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,                        # Limit checkpoints to save disk space\n",
        ")\n",
        "\n",
        "def data_collator(features):\n",
        "    input_features = [feature[\"input_features\"] for feature in features]\n",
        "    labels = [feature[\"labels\"] for feature in features]\n",
        "    max_label_length = max(len(l) for l in labels)\n",
        "    labels = [l + [tokenizer.pad_token_id] * (max_label_length - len(l)) for l in labels]\n",
        "    batch = {\n",
        "        \"input_features\": torch.tensor(input_features, dtype=torch.float32),\n",
        "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "    }\n",
        "    return batch"
      ],
      "metadata": {
        "id": "mIzFIDbDM0IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7: Train model**"
      ],
      "metadata": {
        "id": "lthOmcjbM2zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Baseline evaluation\n",
        "print(\"Baseline Evaluation (before training):\")\n",
        "baseline_results = trainer.evaluate()\n",
        "print(f\"Baseline WER: {baseline_results.get('eval_wer', 'N/A'):.2f}%\")\n",
        "\n",
        "trainer.train()\n",
        "print(\"‚úÖ Training finished.\")\n",
        "\n",
        "# Cell 8: Evaluate and save model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Final Evaluation Results: WER = {eval_results.get('eval_wer', 'N/A'):.2f}%\")\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"fa\", task=\"transcribe\")\n",
        "processor.feature_extractor = feature_extractor\n",
        "processor.tokenizer = tokenizer\n",
        "model.save_pretrained(output_dir)\n",
        "processor.save_pretrained(output_dir)\n",
        "\n",
        "# Verify checkpoint\n",
        "if os.path.exists(os.path.join(output_dir, \"pytorch_model.bin\")):\n",
        "    print(f\"‚úÖ Checkpoint verified: pytorch_model.bin exists in {output_dir}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Checkpoint verification failed: pytorch_model.bin not found in {output_dir}\")\n",
        "\n",
        "print(f\"‚úÖ Model & processor saved in {output_dir}\")"
      ],
      "metadata": {
        "id": "2v-cvk_VM7IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9: Transcription function**"
      ],
      "metadata": {
        "id": "oKY-h8DOM-57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(audio_path, model_path=output_dir, device=device):\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(model_path).to(device)\n",
        "    processor = WhisperProcessor.from_pretrained(model_path)\n",
        "    audio, sampling_rate = sf.read(audio_path)\n",
        "    if sampling_rate != 16000:\n",
        "        audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=16000)\n",
        "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = model.generate(inputs[\"input_features\"])\n",
        "    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
        "    return transcription"
      ],
      "metadata": {
        "id": "56rqiOP3NCJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10: Final report**"
      ],
      "metadata": {
        "id": "EQl-j-SqNEYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìã Final Project Report\")\n",
        "print(\"=\"*50)\n",
        "if os.path.exists(output_dir) and os.path.isfile(os.path.join(output_dir, \"pytorch_model.bin\")):\n",
        "    print(\"‚úÖ Model successfully trained and saved\")\n",
        "    print(\"Directory contents:\")\n",
        "    for item in os.listdir(output_dir):\n",
        "        print(f\"  - {item}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Model was not trained or saved\")\n",
        "print(f\"\\nüî∏ Model: Whisper-Tiny for Persian\")\n",
        "print(f\"üî∏ Training data: {len(train_dataset)} samples\")\n",
        "print(f\"üî∏ Evaluation data: {len(eval_dataset)} samples\")\n",
        "print(f\"üî∏ Device: {str(device).upper()}\")\n",
        "print(f\"üî∏ Word Error Rate (WER): {eval_results.get('eval_wer', 'N/A'):.2f}%\")\n",
        "print(\"üéâ Project completed!\")"
      ],
      "metadata": {
        "id": "4t0FoUcYL6iM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}